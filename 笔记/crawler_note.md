## çˆ¬å–ç½‘ç«™æ€è·¯

1.å…ˆç¡®å®šæ˜¯å¦ä¸ºåŠ¨æ€åŠ è½½ç½‘ç«™  ->  é€šè¿‡æŸ¥çœ‹ç½‘é¡µæºä»£ç ï¼Œæ¥ç¡®å®šæ˜¯å¦æœ‰è‡ªå·±éœ€è¦çš„ä¿¡æ¯(æ³¨æ„ï¼šæœ‰äº›ç½‘é¡µè™½ç„¶æ˜¯é™æ€ç½‘é¡µï¼Œä½†æ˜¯éšç€ä¸‹æ‹‰æ¡†çš„ç§»åŠ¨ï¼Œæœ‰äº›æ ‡ç­¾çš„classå±æ€§ä¼šå‘ç”Ÿæ”¹å˜ã€‚æ­¤æ—¶ï¼Œé€šå¸¸æ˜¯åŸæ¥çš„classå±æ€§å€¼æ˜¯å¯ä»¥å®šä½å…ƒç´ çš„ï¼)
2.æ‰¾URLè§„å¾‹
3.æ­£åˆ™è¡¨è¾¾å¼
4.å®šä¹‰ç¨‹åºæ¡†æ¶ï¼Œè¡¥å…¨å¹¶æµ‹è¯•ä»£ç 

## ç¬¬ä¸€æ¬¡å®‰è£…mysqlæ•°æ®åº“ä¹‹åçš„æ³¨æ„äº‹é¡¹ï¼š

1.ç¬¬ä¸€æ¬¡ç™»å½•ä½¿ç”¨sudo mysql,å¦åˆ™ç™»å½•ä¸ä¸Š;
2.å¦‚æœä½¿ç”¨sudo mysqlä¹Ÿç™»å½•ä¸ä¸Šçš„è¯ï¼Œæ‰¾åˆ°mysqld.cnfé…ç½®æ–‡ä»¶ï¼Œåœ¨[mysqld]ä¸‹æ·»åŠ skip-grant-tablesï¼Œä¿å­˜å³å¯ã€‚æ³¨æ„è¦ä¿®æ”¹æ–‡ä»¶æƒé™ï¼Œä¿å­˜å®Œæˆä¹‹åè¦æŠŠæ–‡ä»¶æƒé™å†æ”¹å›æ¥;
3.ç„¶åè¿›å…¥mysqlä¸­ï¼Œuse mysql,æ‰¾åˆ°userè¡¨;
4.è¾“å…¥update user set authentication_string=password('æ–°å¯†ç ') where user='root';
ã€€ã€€ã€€ã€€flush privileges;
5.æ›´æ”¹å¥½æ–°çš„å¯†ç ä¹‹åï¼Œé€€å‡ºmysql;
6.å†æŠŠmysqld.cnfä¸­çš„skip-grant-tablesæ³¨é‡Šæ‰;
7.é‡å¯mysql;
8.ç„¶åå°±å¯ä»¥æ­£å¸¸è¿›å…¥mysqläº†;
æ³¨æ„ï¼šå¦‚æœä¸Šè¿°æ–¹æ³•è¿˜æ˜¯ä¸è¡Œï¼Œé‚£å°±æŠŠç°æœ‰çš„rootè´¦æˆ·åˆ é™¤ï¼Œç„¶åé‡æ–°åˆ›å»ºä¸€ä¸ªHost=%çš„rootè´¦æˆ·ï¼Œpluginä¸ºç©º;
       å†æŠŠæ‰€æœ‰çš„æƒé™åŠ ç»™æ–°åˆ›å»ºçš„rootè´¦æˆ·;

## çˆ¬è™«å¢é‡çˆ¬å–çš„æ€è·¯

1.MySQLä¸­æ–°å»ºè¡¨ request_fingerï¼Œå­˜å‚¨æ‰€æœ‰çˆ¬å–è¿‡çš„é“¾æ¥çš„æŒ‡çº¹;
2.åœ¨çˆ¬å–ä¹‹å‰ï¼Œå…ˆåˆ¤æ–­è¯¥æŒ‡çº¹æ˜¯å¦çˆ¬å–è¿‡ï¼Œå¦‚æœçˆ¬å–è¿‡ï¼Œåˆ™ä¸å†ç»§ç»­çˆ¬å–;

## ä½¿ç”¨requestsåº“é‡åˆ°å­—ç¬¦ç¼–ç é—®é¢˜çš„å¯¹åº”

ä¸‹é¢è¿™ç§å†™æ³•å¯ä»¥è§„é¿å­—ç¬¦ç¼–ç é”™è¯¯çš„æƒ…å†µ

html = requests.get(url=url,headers=headers).content.decode()
html = requests.get(url=url,headers=headers).text

## ä½¿ç”¨osåˆ›å»ºæ–‡ä»¶å¤¹çš„ä¸¤ç§æ–¹æ³•

å¦‚æœä½¿ç”¨os.mkdir()ï¼Œåˆ™ä¸èƒ½é€’å½’åˆ›å»ºï¼Œå³å¦‚æœå­˜åœ¨ä¸¤çº§æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨ï¼Œå°±æ— æ³•ä½¿ç”¨ã€‚

è€Œä½¿ç”¨os.makedirs(directory)ï¼Œå°±å¯ä»¥å…¨è¿‡ç¨‹ç”Ÿæˆæ–‡ä»¶å¤¹ã€‚



contains():åŒ¹é…å±æ€§å€¼ä¸­åŒ…å«æŸäº›å­—ç¬¦ä¸²èŠ‚ç‚¹

æŸ¥æ‰¾idå±æ€§å€¼ä¸­åŒ…å«å­—ç¬¦ä¸²"car_"çš„lièŠ‚ç‚¹

//li[contains(@id,'car_')]

## å¦‚æœxpathçš„è§£æè·¯å¾„å¤æ‚ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨//

.//p[@class='name']/a/text()   å³è·å–åœ¨å½“å‰å…ƒç´ èŠ‚ç‚¹ä¸‹çš„ğŸ¥°ï¸åä»£èŠ‚ç‚¹ğŸ¥°ï¸å±æ€§class='name'çš„pèŠ‚ç‚¹ä¸‹çš„aå­—èŠ‚ç‚¹çš„æ–‡æœ¬æ•°æ®ï¼Œä½¿ç”¨//å¯ä»¥è·³è¿‡ä¸­é—´èŠ‚ç‚¹ï¼Œç›´æ¥å®šä½åˆ°pèŠ‚ç‚¹;

ç›®å‰åçˆ¬æ€»ç»“
1.åŸºäºUser-Agentåçˆ¬;
2.å¦‚æœæ•°æ®å‡ºä¸æ¥å¯è€ƒè™‘æ›´æ¢**IE**çš„User-Agentå°è¯•ï¼Œæ•°æ®è¿”å›æœ€æ ‡å‡†;

## urllibåº“ä½¿ç”¨æµç¨‹

ç¼–ç 

params = {
	'':'',
	'':''
}
params = urllib.parse.urlencode(params)
url = baseurl + params  # urlæ‹¼æ¥

#è¯·æ±‚
request = urllib.request.Request(url,headers = headers)
response = urllib.request.urlopen(request)
html = response.read().decode('utf-8')

## requestsæ¨¡å—ä½¿ç”¨æµç¨‹

params = {
	'':'',
	'':''
}
baseurl = 'http://tieba.baidu.com/f?'
html = requests.get(baseurl,params = params,headers = headers).content.decode('utf-8','ignore')

å“åº”å¯¹è±¡reså±æ€§
res.text : å­—ç¬¦ä¸²
res.content : bytes
res.encoding : å­—ç¬¦ç¼–ç   res.encoding = 'utf-8'
res.status_code : HTTPå“åº”ç 
res.url : å®é™…æ•°æ®URLåœ°å€

æ­£åˆ™è§£æreæ¨¡å—
import re

pattern = re.compile(r'æ­£åˆ™è¡¨è¾¾å¼',re.S)
r_list = pattern.findall(html)

----------------------------------------------------------

lxmlè§£æåº“
from lxml import etree

parse_html = etree.HTML(res.text)
r_list = parse_html.xpath('xpathè¡¨è¾¾å¼')

----------------------------------------------------------
xpathè¡¨è¾¾å¼
->åŒ¹é…è§„åˆ™
1.èŠ‚ç‚¹å¯¹è±¡åˆ—è¡¨://divã€//div[@class="student"]ã€//div/a[@title='stu']/span
2.å­—ç¬¦ä¸²åˆ—è¡¨:xpathè¡¨è¾¾å¼ä¸­æœ«å°¾ä¸ºï¼š@srcã€@hrefã€text()

->xpathé«˜çº§
1.åŸºå‡†xpathè¡¨è¾¾å¼ï¼šå¾—åˆ°èŠ‚ç‚¹å¯¹è±¡åˆ—è¡¨
2.for r in [èŠ‚ç‚¹å¯¹è±¡åˆ—è¡¨]:
      username = r.xpath('./xxxxxx')

æ­¤å¤„æ³¨æ„éå†åç»§ç»­xpathä¸€å®šè¦ä»¥:  . å¼€å¤´,ä»£è¡¨å½“å‰èŠ‚ç‚¹

----------------------------------------------------------
## çˆ¬è™«æ—¶æ³¨æ„ï¼š

# æœ€ç»ˆç›®æ ‡: ä¸è¦ä½¿ä½ çš„ç¨‹åºå› ä¸ºä»»ä½•å¼‚å¸¸è€Œç»ˆæ­¢
1.é¡µé¢è¯·æ±‚è®¾ç½®è¶…æ—¶æ—¶é—´,å¹¶ç”¨tryæ•æ‰å¼‚å¸¸ï¼Œè¶…è¿‡æŒ‡å®šæ¬¡æ•°åˆ™æ›´æ¢ä¸‹ä¸€ä¸ªURLåœ°å€;
2.æ‰€æŠ“å–ä»»ä½•æ•°æ®ï¼Œè·å–å…·ä½“æ•°æ®å‰å…ˆåˆ¤æ–­æ˜¯å¦å­˜åœ¨è¯¥æ•°æ®ï¼Œå¯ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              
(é‡è¦ï¼)3.é€šå¸¸çš„ç½‘é¡µçˆ¬å–ï¼Œä½¿ç”¨getå†åŠ ä¸Šheaderså³å¯çˆ¬å–;å¦‚æœè·å–ä¸åˆ°æ•°æ®ï¼Œè¦æ³¨æ„æ£€æŸ¥headersé‡Œçš„å‚æ•°ï¼Œæœ‰äº›å‚æ•°æ˜¯ä¸éœ€è¦çš„ã€‚ä¸€èˆ¬ä¿ç•™cookie,User-Agent,å³å¯ã€‚

# å¤šçº§é¡µé¢æ•°æ®çˆ¬å–æ³¨æ„ï¼š
1.ä¸»çº¿å‡½æ•°ï¼šè§£æä¸€çº§é¡µé¢å‡½æ•°(å°†æ‰€æœ‰æ•°æ®ä»ä¸€çº§é¡µé¢ä¸­è§£æå¹¶æŠ“å–)

----------------------------------------------------------

## å¢é‡çˆ¬è™«å¦‚ä½•å®ç°ï¼Ÿ

1.æ•°æ®åº“ä¸­åˆ›å»ºæŒ‡çº¹è¡¨ï¼Œç”¨æ¥å­˜å‚¨æ¯ä¸ªè¯·æ±‚çš„æŒ‡çº¹;
2.åœ¨æŠ“å–ä¹‹å‰ï¼Œå…ˆåˆ°æŒ‡çº¹è¡¨ä¸­ç¡®è®¤æ˜¯å¦ä¹‹å‰æŠ“å–è¿‡;

## çˆ¬è™«åŸºæœ¬æ­¥éª¤ï¼š

1.æŸ¥çœ‹æ˜¯å¦ä¸ºé™æ€é¡µé¢;
2.ç¡®å®šxpathè¡¨è¾¾å¼;
3.å¯¹æ‰€è¦çˆ¬å–çš„å†…å®¹è¿›è¡Œsource codeæ£€æŸ¥ï¼Œå¦‚æœæºä»£ç éš¾ä»¥åŒºåˆ†ï¼Œå¯ä»¥è¿›è¡Œåœ¨çº¿æ ¼å¼åŒ–çš„å·¥å…·ç”Ÿæˆä¸€ä¸‹ï¼Œç„¶åå†æºä»£ç é‡ŒæŸ¥æ‰¾æ‰€è¦å®šä½çš„å†…å®¹;

## requestsä¸­çš„paramså‚æ•°

ç‰¹ç‚¹ï¼š1.urlä¸ºåŸºå‡†çš„urlåœ°å€ï¼Œä¸åŒ…å«æŸ¥è¯¢å‚æ•°;
            2.è¯¥æ–¹æ³•ä¼šè‡ªåŠ¨å¯¹paramså­—å…¸ç¼–ç ï¼Œç„¶åå’Œurlæ‹¼æ¥;
      
Webå®¢æˆ·ç«¯éªŒè¯å‚æ•°-auth
1.é’ˆå¯¹äºéœ€è¦webå®¢æˆ·ç«¯ç”¨æˆ·åå¯†ç è®¤è¯çš„ç½‘ç«™
2.auth=('username','password') # æ³¨æ„å‚æ•°ç±»å‹ä¸ºä¸€ä¸ªå…ƒç»„ï¼Œå…ƒç»„é‡Œæ˜¯ç”¨æˆ·åå’Œå¯†ç 
3.requests.get(url=url,headers=headers,auth=auth)

## SSLè¯ä¹¦è®¤è¯å‚æ•°-verify

é€‚ç”¨ç½‘ç«™åŠåœºæ™¯

1ã€é€‚ç”¨ç½‘ç«™ï¼šhttpsç±»å‹ç½‘ç«™ä½†æ˜¯æ²¡æœ‰ç»è¿‡è¯ä¹¦è®¤è¯æœºæ„è®¤è¯çš„ç½‘ç«™;
2ã€é€‚ç”¨åœºæ™¯ï¼šæŠ›å‡ºSSLErrorå¼‚å¸¸åˆ™è€ƒè™‘ä½¿ç”¨æ­¤å‚æ•°;

# å‚æ•°ç±»å‹
1ã€verify=True(é»˜è®¤):æ£€æŸ¥è¯ä¹¦è®¤è¯
2ã€verify=False(å¸¸ç”¨):å¿½ç•¥è¯ä¹¦è®¤è¯

# ç¤ºä¾‹
response = requests.get(
	url = url,
	params = params,
	headers = headers,
	verify = False
)

-------------------------------------------------------------------
ä»£ç†å‚æ•°-proxies
#å®šä¹‰
1.å®šä¹‰ï¼šä»£æ›¿ä½ åŸæ¥çš„IPåœ°å€å»å¯¹æ¥ç½‘ç»œçš„IPåœ°å€;
2.ä½œç”¨ï¼šéšè—è‡ªèº«çœŸå®IPï¼Œé¿å…è¢«å°;

æ™®é€šä»£ç†
# è·å–ä»£ç†IPç½‘ç«™
è¥¿åˆºä»£ç†ã€å¿«ä»£ç†ã€å…¨ç½‘ä»£ç†ã€ä»£ç†ç²¾çµã€â€¦â€¦

# å‚æ•°ç±»å‹
1ã€è¯­æ³•ç»“æ„
	proxies = {
		'åè®®':'åè®®://IP:ç«¯å£å·'
	}
2ã€ç¤ºä¾‹
	proxies = {
		'http':'http://IP:ç«¯å£å·',
		'https':'https://IP:ç«¯å£å·'
}		

é«˜åŒ¿ï¼šé«˜åº¦åŒ¿åï¼Œwebç«™ç‚¹åªèƒ½çœ‹åˆ°ä»£ç†IP -> ä¼˜å…ˆé€‰æ‹©
é€æ˜ï¼šWebç«™ç‚¹èƒ½çœ‹åˆ°ä»£ç†IPå’Œç”¨æˆ·è‡ªèº«çœŸå®IP
æ™®é€š:   Webç«™ç‚¹èƒ½çœ‹åˆ°ä»£ç†IPå’ŒçŸ¥é“æœ‰äººé€šè¿‡ä»£ç†IPè®¿é—®ï¼Œä½†æ˜¯ä¸çŸ¥é“ç”¨æˆ·çœŸå®IP

æ€è€ƒï¼šå»ºç«‹ä¸€ä¸ªè‡ªå·±çš„ä»£ç†IPæ± ï¼Œéšæ—¶æ›´æ–°ç”¨æ¥æŠ“å–ç½‘ç«™æ•°æ®
1ã€ä»è¥¿åˆºä»£ç†IPç½‘ç«™ä¸Šï¼ŒæŠ“å–å…è´¹ä»£ç†IP;
2ã€æµ‹è¯•æŠ“å–çš„IPï¼Œå¯ç”¨çš„ä¿å­˜åœ¨æ–‡ä»¶ä¸­;

-------------------------------------------------------------------

å¸¸è§çš„åçˆ¬æœºåˆ¶åŠå¤„ç†æ–¹å¼
1ã€Headersåçˆ¬è™«ï¼šCookieã€Refererã€User-Agent
   è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡F12è·å–headersï¼Œä¼ ç»™requests.get()æ–¹æ³•
2ã€IPé™åˆ¶ï¼šç½‘ç«™æ ¹æ®IPåœ°å€è®¿é—®é¢‘ç‡è¿›è¡Œåçˆ¬ï¼ŒçŸ­æ—¶é—´å†…é™åˆ¶IPè®¿é—®
   è§£å†³æ–¹æ¡ˆï¼š  1ã€æ„é€ è‡ªå·±IPä»£ç†æ± ï¼Œæ¯æ¬¡è®¿é—®éšæœºé€‰æ‹©ä»£ç†ï¼Œç»å¸¸æ›´æ–°ä»£ç†æ± ;
            			 2ã€è´­ä¹°å¼€æ”¾ä»£ç†æˆ–ç§å¯†ä»£ç†IP;
             			3ã€é™ä½çˆ¬å–çš„é€Ÿåº¦;
3ã€User-Agenté™åˆ¶ï¼šç±»ä¼¼äºIPé™åˆ¶
   è§£å†³æ–¹æ¡ˆï¼šæ„é€ è‡ªå·±çš„User-Agentæ± ï¼Œæ¯æ¬¡è®¿é—®éšæœºé€‰æ‹©;
4ã€å¯¹æŸ¥è¯¢å‚æ•°æˆ–Formè¡¨å•æ•°æ®è®¤è¯(saltã€sign)
   è§£å†³æ–¹æ¡ˆï¼šæ‰¾åˆ°JSæ–‡ä»¶ï¼Œåˆ†æJSå¤„ç†æ–¹æ³•ï¼Œç”¨PythonæŒ‰åŒæ ·æ–¹æ³•å¤„ç†;
5ã€å¯¹å“åº”å†…å®¹åšå¤„ç†
   è§£å†³æ–¹æ¡ˆï¼šæ‰“å°å¹¶æŸ¥çœ‹å“åº”å†…å®¹ï¼Œç”¨xpathæˆ–æ­£åˆ™åšå¤„ç†;

-------------------------------------------------------------------

## æ§åˆ¶å°æŠ“åŒ…

æ‰“å¼€æ–¹å¼åŠå¸¸ç”¨é€‰é¡¹

1ã€æ‰“å¼€æµè§ˆå™¨ï¼ŒF12æ‰“å¼€æ§åˆ¶å°ï¼Œæ‰¾åˆ°networké€‰é¡¹å¡;
2ã€æ§åˆ¶å°å¸¸ç”¨é€‰é¡¹ï¼š
   *Network:æŠ“å–ç½‘ç»œæ•°æ®åŒ…
   	1ã€ALL:æŠ“å–æ‰€æœ‰çš„ç½‘ç»œæ•°æ®åŒ…;
   	2ã€XHR:æŠ“å–å¼‚æ­¥åŠ è½½çš„ç½‘ç»œæ•°æ®åŒ…;
   	3ã€JS:æŠ“å–æ‰€æœ‰çš„JSæ–‡ä»¶;
   *Sources:æ ¼å¼åŒ–è¾“å‡ºå¹¶æ‰“æ–­ç‚¹è°ƒè¯•JavaScriptä»£ç ï¼ŒåŠ©äºåˆ†æçˆ¬è™«ä¸­ä¸€äº›å‚æ•°ï¼ˆé‡è¦ï¼ï¼ï¼ï¼ï¼‰
   *Console:äº¤äº’æ¨¡å¼ï¼Œå¯å¯¹JavaScriptä¸­çš„ä»£ç è¿›è¡Œæµ‹è¯•;
3ã€æŠ“å–å…·ä½“ç½‘ç»œæ•°æ®åŒ…å
   1ã€å•å‡»å·¦ä¾§ç½‘ç»œæ•°æ®åŒ…åœ°å€ï¼Œè¿›å…¥æ•°æ®åŒ…è¯¦æƒ…ï¼ŒæŸ¥çœ‹å³ä¾§;
   2ã€å³ä¾§:
       1ã€Headers:æ•´ä¸ªè¯·æ±‚ä¿¡æ¯
           Generalã€Response Headersã€Request Headersã€Query Stringã€Form Data
       2ã€Preview:å¯¹å“åº”å†…å®¹è¿›è¡Œé¢„è§ˆ
       3ã€Response:å“åº”å†…å®¹
4ã€JSæŠ“åŒ…çš„æ—¶å€™ï¼Œå¦‚æœé‡åˆ°æ— æ³•è¾¨åˆ«çš„å˜é‡ï¼Œå¯ä»¥è¿›è¡Œæ–­ç‚¹è°ƒè¯•ï¼Œè¿›è¡Œä¸€ä¸ªè¾“å…¥æ“ä½œï¼Œç„¶åæŸ¥çœ‹æ— æ³•è¾¨åˆ«å˜é‡çš„å€¼;  (é‡è¦ï¼ï¼ï¼ï¼‰
5ã€æŠ“åŒ…æ—¶ï¼Œåœ¨FormDataä¸­å¦‚æœå‘ç°salt,signè¿™æ ·çš„å­—æ®µï¼Œå°±è¦æé«˜è­¦æƒ•ï¼Œä¼šå­˜åœ¨åçˆ¬æ“ä½œï¼Œéœ€è¦è‡ªå·±ç¼–ç ç”Ÿæˆå¿…è¦çš„æ•°æ®ã€‚å…·ä½“è¦æŸ¥çœ‹jsçš„ç¼–ç æ–¹å¼ï¼Œç„¶åç¡®å®šå¦‚ä½•é€šè¿‡pythonè„šæœ¬ç”Ÿæˆæ•°æ®ï¼›

æ‰©å±•
1.å»ºç«‹å¢é‡çˆ¬è™« - ç½‘ç«™æœ‰æ›´æ–°æ—¶æŠ“å–ï¼Œå¦åˆ™ä¸æŠ“
	#æ•°æ®åº“ä¸­å»ºç«‹versionè¡¨ï¼Œå­˜å‚¨æŠ“å–è¿‡çš„urlåœ°å€
	
2.æ‰€æŠ“æ•°æ®å­˜åˆ°æ•°æ®åº“ï¼ŒæŒ‰ç…§å±‚çº§å…³ç³»åˆ†è¡¨å­˜å‚¨ - çœã€å¸‚ã€å¿è¡¨
	åŒ—äº¬å¸‚  åŒ—äº¬å¸‚ ä¸œåŸåŒº 
	å¹¿ä¸œçœ  å¹¿å·å¸‚ è¶Šç§€åŒº
	
-------------------------------------------------------------------

## åŠ¨æ€åŠ è½½æ•°æ®æŠ“å–-Ajax

# ç‰¹ç‚¹
1ã€å³é”® -> æŸ¥çœ‹ç½‘é¡µæºç ä¸­æ²¡æœ‰å…·ä½“æ•°æ®;
2ã€æ»šåŠ¨é¼ æ ‡æ»‘è½®æˆ–å…¶ä»–åŠ¨ä½œæ—¶åŠ è½½ï¼Œæˆ–è€…é¡µé¢å±€éƒ¨åˆ·æ–°;

# æŠ“å–
1ã€F12æ‰“å¼€æ§åˆ¶å°ï¼Œé¡µé¢åŠ¨ä½œæŠ“å–ç½‘ç»œæ•°æ®åŒ…;
2ã€æŠ“å–jsonæ–‡ä»¶URLåœ°å€;
# æ§åˆ¶å°ä¸­ XHR : å¼‚æ­¥åŠ è½½çš„æ•°æ®åŒ…
# XHR -> QueryStringParameters(æŸ¥è¯¢å‚æ•°)

-------------------------------------------------------------------

æœ‰é“ç¿»è¯‘çˆ¬è™«æ¡ˆä¾‹è¯¦ç»†åˆ†æ
1ã€æ‰“å¼€é¦–é¡µ
2ã€å‡†å¤‡æŠ“åŒ…:F12å¼€å¯æ§åˆ¶å°
3ã€å¯»æ‰¾åœ°å€
	é¡µé¢ä¸­è¾“å…¥ç¿»è¯‘å•è¯ï¼Œæ§åˆ¶å°ä¸­æŠ“å–åˆ°ç½‘ç»œæ•°æ®åŒ…ï¼ŒæŸ¥æ‰¾å¹¶åˆ†æè¿”å›ç¿»è¯‘æ•°æ®çš„åœ°å€
4ã€å‘ç°è§„å¾‹
	æ‰¾åˆ°è¿”å›å…·ä½“æ•°æ®çš„åœ°å€ï¼Œåœ¨é¡µé¢ä¸­å¤šè¾“å…¥å‡ ä¸ªå•è¯ï¼Œæ‰¾åˆ°å¯¹åº”URLåœ°å€ï¼Œåˆ†æå¯¹æ¯”Network - ALL(æˆ–è€…XHRï¼‰- Form Dataï¼Œå‘ç°å¯¹åº”è§„å¾‹
5ã€å¯»æ‰¾JSæ–‡ä»¶
	å³ä¸Šè§’ ... -> Search -> æœç´¢å…³é”®å­— -> å•å‡» -> è·³è½¬åˆ°Sources,å·¦ä¸‹è§’æ ¼å¼åŒ–ç¬¦å·{}

-------------------------------------------------------------------

çŸ¥è¯†ç‚¹å›é¡¾
*é˜Ÿåˆ—
#å¯¼å…¥æ¨¡å—
from queue import Queue

#ä½¿ç”¨
q = Queue()
q.put(url)
q.get() # å½“é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œé˜»å¡
q.get(block=True,timeout=3) # è¶…è¿‡3ç§’æŠ›å¼‚å¸¸
q.get(block=False) # ä¸ºç©ºç›´æ¥æŠ›å¼‚å¸¸
q.empty() # åˆ¤æ–­é˜Ÿåˆ—æ˜¯å¦ä¸ºç©ºï¼ŒTrue/False

## *çº¿ç¨‹æ¨¡å—

å¯¼å…¥æ¨¡å—

from threading import Thread

# ä½¿ç”¨æµç¨‹
t = Thread(target=å‡½æ•°å) # åˆ›å»ºçº¿ç¨‹å¯¹è±¡
t.start()  # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
t.join()  # é˜»å¡ç­‰å¾…å›æ”¶çº¿ç¨‹

# å¦‚ä½•åˆ›å»ºå¤šçº¿ç¨‹ï¼Ÿ
æŠ€å·§ï¼šå¤šçº¿ç¨‹çˆ¬è™«ï¼Œè¦æ³¨æ„åˆ›å»ºä¸€ä¸ªé˜Ÿåˆ—å¯¹è±¡ï¼Œfrom queue import Queueï¼Œç”¨æ¥å­˜å‚¨æ‰€æœ‰çš„URLã€‚
     å¯¹åº”çˆ¬å–ç½‘é¡µURLæ—¶ï¼Œåˆ†åˆ«ä¸€ä¸ªçº¿ç¨‹åˆ†é…ä¸€ä¸ªURLã€‚

-------------------------------------------------------------------

å¤šçº¿ç¨‹çˆ¬è™«æµç¨‹ï¼š
1ã€å°†å¾…çˆ¬å–çš„URLåœ°å€å­˜æ”¾åˆ°é˜Ÿåˆ—ä¸­;
2ã€å¤šä¸ªçº¿ç¨‹ä»é˜Ÿåˆ—ä¸­è·å–åœ°å€ï¼Œè¿›è¡Œæ•°æ®æŠ“å–;
3ã€æ³¨æ„è·å–åœ°å€è¿‡ç¨‹ä¸­ç¨‹åºé˜»å¡é—®é¢˜;
	# å†™æ³•:
â€‹	while True:
â€‹	    try:
â€‹	        url = q.get(block=True,timeout=3)
â€‹		xxx xxx
â€‹	    except Exception as e:
â€‹	    	break


*å°†æŠ“å–æ•°æ®ä¿å­˜åˆ°åŒä¸€æ–‡ä»¶
# æ³¨æ„å¤šçº¿ç¨‹å†™å…¥çš„çº¿ç¨‹é”é—®é¢˜
from threading import Lock
lock = Lock()
lock.acquire()
pythonä»£ç å—
lock.release()

## çº¿ç¨‹å®‰å…¨

ä¸‹é¢æœ‰ä¸€ä¸ªæ•°å€¼numåˆå§‹å€¼ä¸º0,æˆ‘ä»¬å¼€å¯2æ¡çº¿ç¨‹:

-> çº¿ç¨‹1å¯¹numè¿›è¡Œä¸€åƒä¸‡æ¬¡+1çš„æ“ä½œ

-> çº¿ç¨‹2å¯¹numè¿›è¡Œä¸€åƒä¸‡æ¬¡-1çš„æ“ä½œ

*# ç»“æœä¸‰æ¬¡é‡‡é›†*
*# num result : 669214*
*# num result : -1849179*
*# num result : -525674*

ä¸Šé¢å°±æ˜¯éå¸¸å¥½çš„æ¡ˆä¾‹,æƒ³è¦è§£å†³è¿™ä¸ªé—®é¢˜å°±å¿…é¡»é€šè¿‡é”æ¥ä¿éšœçº¿ç¨‹åˆ‡æ¢çš„æ—¶æœºã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨PythonåŸºæœ¬æ•°æ®ç±»å‹ä¸­listã€typleã€dictæœ¬èº«å°±æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œæ‰€ä»¥å¦‚æœæœ‰å¤šä¸ªçº¿ç¨‹å¯¹è¿™3ç§å®¹å™¨åšæ“ä½œæ—¶ï¼Œæˆ‘ä»¬ä¸å¿…è€ƒè™‘çº¿ç¨‹å®‰å…¨çš„é—®é¢˜ã€‚

## é”çš„ä½œç”¨

é”æ˜¯Pythonæä¾›ç»™æˆ‘ä»¬èƒ½å¤Ÿè‡ªè¡Œæ“æ§çº¿ç¨‹åˆ‡æ¢çš„ä¸€ç§æ‰‹æ®µï¼Œä½¿ç”¨é”å¯ä»¥è®©çº¿ç¨‹çš„åˆ‡æ¢å˜å¾—æœ‰åºã€‚

ä¸€æ—¦çº¿ç¨‹çš„åˆ‡æ¢å˜å¾—æœ‰åºåï¼Œå„ä¸ªçº¿ç¨‹ä¹‹é—´å¯¹æ•°æ®çš„è®¿é—®ã€ä¿®æ”¹å°±å˜å¾—å¯æ§ï¼Œæ‰€ä»¥è‹¥è¦ä¿è¯çº¿ç¨‹å®‰å…¨ï¼Œå°±å¿…é¡»ä½¿ç”¨é”ã€‚

threadingæ¨¡å—ä¸­æä¾›äº†5ç§æœ€å¸¸è§çš„é”ï¼ŒæŒ‰ç…§åŠŸèƒ½åˆ’åˆ†å¦‚ä¸‹:

- åŒæ­¥é”:lock(ä¸€æ¬¡åªèƒ½æ”¾è¡Œä¸€ä¸ª)

- é€’å½’é”:rlock(ä¸€æ¬¡åªèƒ½æ”¾è¡Œä¸€ä¸ª)

- æ¡ä»¶é”:condition(ä¸€æ¬¡å¯ä»¥æ”¾è¡Œä»»æ„ä¸ª)

- äº‹ä»¶é”:event(ä¸€æ¬¡å…¨éƒ¨æ”¾è¡Œ)

- ä¿¡å·é‡é”:semaphore(ä¸€æ¬¡å¯ä»¥æ”¾è¡Œç‰¹å®šä¸ª)

  

-------------------------------------------------------------------

cookieæ¨¡æ‹Ÿç™»å½•
é€‚ç”¨ç½‘ç«™åŠåœºæ™¯ -> æŠ“å–éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®çš„é¡µé¢

cookieå’Œsessionæœºåˆ¶
# httpåè®®ä¸ºæ— è¿æ¥åè®®
cookie:å­˜æ”¾åœ¨å®¢æˆ·ç«¯æµè§ˆå™¨
session:å­˜æ”¾åœ¨webæœåŠ¡å™¨

æ–¹æ³•1ï¼šheadersä¸­åŠ å…¥å¯¹åº”cookie

æ–¹æ³•2ï¼š
åŸç† -> 1ã€æŠŠæŠ“å–åˆ°çš„cookieå¤„ç†ä¸ºå­—å…¸;
        2ã€ä½¿ç”¨requests.get()ä¸­çš„å‚æ•°:cookies;
	res = requests.get(
		url = url,
		params = params,
		auth = auth,
		proxies = proxies,
		headers = headers,
		timeout = 5,
		cookies = cookies
	)
        
å¤„ç†cookieä¸ºå­—å…¸ï¼Œç”¨è¿‡requests.get(cookies=cookies)å‘èµ·è¯·æ±‚

æ–¹æ³•3ï¼š
åŸç†æ€è·¯åŠå®ç°

#1.æ€è·¯
requestsæ¨¡å—æä¾›äº†sessionç±»ï¼Œæ¥å®ç°å®¢æˆ·ç«¯å’ŒæœåŠ¡ç«¯çš„ä¼šè¯ä¿æŒ;

#2.åŸç†
1ã€å®ä¾‹åŒ–sessionå¯¹è±¡
   session = requests.session()
2ã€è®©sessionå¯¹è±¡å‘é€getæˆ–è€…
è¯·æ±‚
   res = session.post(url=url,data = data,headers = headers)
   res = session.get(url = url, headers = headers)

#3.æ€è·¯æ¢³ç†
æµè§ˆå™¨åŸç†ï¼šè®¿é—®éœ€è¦ç™»å½•çš„é¡µé¢ä¼šå¸¦ç€ä¹‹å‰ç™»å½•è¿‡çš„cookie;
ç¨‹åºåŸç†: åŒæ ·å¸¦ç€ä¹‹å‰ç™»å½•çš„cookieå»è®¿é—® - ç”±sessionå¯¹è±¡å®Œæˆ
1ã€å®ä¾‹åŒ–sessionå¯¹è±¡
2ã€ç™»å½•ç½‘ç«™ï¼šsessionå¯¹è±¡å‘é€è¯·æ±‚ï¼Œç™»å½•å¯¹åº”ç½‘ç«™ï¼ŒæŠŠcookieä¿å­˜åœ¨sessionå¯¹è±¡ä¸­
3ã€è®¿é—®é¡µé¢ï¼šsessionå¯¹è±¡è¯·æ±‚éœ€è¦ç™»å½•æ‰èƒ½è®¿é—®çš„é¡µé¢ï¼Œsessionèƒ½å¤Ÿè‡ªåŠ¨æºå¸¦ä¹‹å‰çš„è¿™ä¸ªcookieï¼Œè¿›è¡Œè¯·æ±‚

å…·ä½“æ­¥éª¤ï¼š
1ã€å¯»æ‰¾Formè¡¨å•æäº¤åœ°å€ - å¯»æ‰¾ç™»å½•æ—¶POSTçš„åœ°å€
	æŸ¥çœ‹ç½‘é¡µæºç ï¼ŒæŸ¥çœ‹Formè¡¨å•ï¼ŒæŸ¥æ‰¾actionå¯¹åº”çš„åœ°å€ï¼šhttp://www.renren.com/PLogin.do
2ã€å‘é€ç”¨æˆ·åå’Œå¯†ç ä¿¡æ¯åˆ°POSTçš„åœ°å€
	* ç”¨æˆ·åå’Œå¯†ç ä¿¡æ¯ä»¥ä»€ä¹ˆæ–¹å¼å‘é€? -- å­—å…¸
	é”® :  <input>æ ‡ç­¾ä¸­çš„nameå€¼ (email,password)
	å€¼ :  çœŸå®çš„ç”¨æˆ·åå’Œå¯†ç  post_data = {'email':'','password':''}
	

session = requests.session()
session.post(url=url,data=data)

-------------------------------------------------------------------

phantomjsæµè§ˆå™¨
# å®šä¹‰
	æ— ç•Œé¢æµè§ˆå™¨(åˆç§°æ— å¤´æµè§ˆå™¨),åœ¨å†…å­˜ä¸­è¿›è¡Œé¡µé¢åŠ è½½ï¼Œé«˜æ•ˆã€‚

Tip:å¦‚ä½•å¿«é€Ÿåœ¨windowsä¸­æ‰¾åˆ°æŸæ–‡ä»¶çš„ä¿å­˜ä½ç½®ï¼Ÿ  -> CMDæ‰“å¼€DOSï¼Œè¾“å…¥where + æ–‡ä»¶åï¼Œå›è½¦ï¼Œå³å¯

Linuxç³»ç»Ÿä¸­ä¸‹è½½geckodriver.tar.gz
1ã€ä¸‹è½½åè§£å‹ -> tar -zxvf geckodriver.tar.gz
2ã€æ‹·è´è§£å‹åæ–‡ä»¶åˆ° /usr/bin/ (æ·»åŠ ç¯å¢ƒå˜é‡) -> sudo cp geckodriver /usr/bin/
3ã€æ›´æ”¹æƒé™  ->  sudo -i; cd /usr/bin/;  chmod 777 geckodriver;
4ã€æ›´æ”¹å®Œæ¯•åï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤æŸ¥çœ‹geckodriveræ–‡ä»¶çš„æƒé™ -> ls -l geckodriver;
(é‡è¦)Tip:å¦‚ä½•åˆ¤æ–­ç½‘é¡µæ˜¯å¦ä¸ºæœ€åä¸€é¡µï¼Ÿ -> self.browser.page_source.find('pn-next disabled') == -1?   -1è¯´æ˜æ²¡æ‰¾åˆ°ï¼Œä¸æ˜¯æœ€åä¸€ä¸ªé¡µï¼Œæ•…ç‚¹å‡» ä¸‹ä¸€é¡µ æŒ‰é’®

-------------------------------------------------------------------

å›é¡¾

cookieæ¨¡æ‹Ÿç™»å½•
1ã€é€‚ç”¨ç½‘ç«™ç±»å‹ï¼šçˆ¬å–ç½‘ç«™é¡µé¢æ—¶éœ€è¦ç™»å½•åæ‰èƒ½è®¿é—®ï¼Œå¦åˆ™è·å–ä¸åˆ°é¡µé¢çš„å®é™…å“åº”æ•°æ®;
2ã€æ–¹æ³•1(åˆ©ç”¨cookie)
    1ã€å…ˆç™»å½•æˆåŠŸ1æ¬¡ï¼Œè·å–åˆ°æºå¸¦ç™»å½•ä¿¡æ¯çš„cookie(å¤„ç†headers)
    2ã€åˆ©ç”¨å¤„ç†çš„headerså‘URLåœ°å€å‘è¯·æ±‚
3ã€æ–¹æ³•2(åˆ©ç”¨requests.get()ä¸­cookieså‚æ•°)
    1ã€å…ˆç™»å½•æˆåŠŸä¸€æ¬¡ï¼Œè·å–åˆ°cookieï¼Œå¤„ç†ä¸ºå­—å…¸
    2ã€res=requests.get(xxx,cookies=cookies)
4ã€æ–¹æ³•3(åˆ©ç”¨sessionä¼šè¯ä¿æŒ)
    1ã€å®ä¾‹åŒ–sessionå¯¹è±¡
       session = requests.session()
    2ã€å…ˆpost :
       session.post(post_url,data = post_data,headers = headers)
       1ã€ç™»å½•ï¼Œæ‰¾åˆ°POSTåœ°å€: form -> actionå¯¹åº”åœ°å€
       2ã€å®šä¹‰å­—å…¸ï¼Œåˆ›å»ºsessionå®ä¾‹å‘é€è¯·æ±‚ â€”> å­—å…¸key:<input> æ ‡ç­¾ä¸­nameçš„å€¼(email,password)
       					   â€”â€”> post_data = {'email':'','password':''}
       3ã€å†get: session.get(url,headers=headers)   

ä¸‰ä¸ªæ± å­
1ã€User-Agentæ± 
2ã€ä»£ç†IPæ± 
3ã€cookieæ± 

-------------------------------------------------------------------
chromedriverè®¾ç½®æ— ç•Œé¢æ¨¡å¼
from selenium import webdriver

options = webdriver.ChromeOptions()
#æ·»åŠ æ— ç•Œé¢å‚æ•°
options.add_argument('--headless')

browser = webdriver.Chrome(options=options)
browser.get('http://www.baidu.com/')
browser.save_screenshot('baidu.png')
       					
#å¯¼å…¥é¼ æ ‡äº‹ä»¶ç±»
from selenium.webdriver import ActionChains

driver = webdriver.Chrome()
driver.get('https://www.baidu.com/')

# ç§»åŠ¨åˆ° è®¾ç½®ï¼Œperform()æ˜¯çœŸæ­£æ‰§è¡Œæ“ä½œï¼Œå¿…é¡»æœ‰
element = driver.find_element(By.XPATH,'//*[@id="ul"]/a[8]')
ActionChains(driver).move_to_element(element).perform()

# å•å‡»ï¼Œå¼¹å‡ºçš„Ajaxå…ƒç´ ï¼Œæ ¹æ®é“¾æ¥èŠ‚ç‚¹çš„æ–‡æœ¬å†…å®¹æŸ¥æ‰¾
driver.find_element(By.XPATH,'text()="é«˜çº§æœç´¢"').click()

-------------------------------------------------------------------

selenium -  åˆ‡æ¢é¡µé¢

# é€‚ç”¨ç½‘ç«™ -> é¡µé¢ä¸­ç‚¹å¼€é“¾æ¥å‡ºç°æ–°çš„é¡µé¢ï¼Œä½†æ˜¯æµè§ˆå™¨å¯¹è±¡browserè¿˜æ˜¯ä¹‹å‰é¡µé¢å¯¹è±¡

# åº”å¯¹æ–¹æ¡ˆ 
   è·å–å½“å‰æ‰€æœ‰å¥æŸ„(çª—å£) -> all_handles = browser.window_handles
   åˆ‡æ¢browseråˆ°æ–°çš„çª—å£,è·å–æ–°çª—å£çš„å¯¹è±¡ -> browser.switch_to.window(all_handles[1])

-------------------------------------------------------------------

mysqlæ•°æ®åº“selectæŸ¥è¯¢è¯­å¥ä¸¾ä¾‹
sel = "select * from version where url=%s"
result = self.cursor.execute(sel,[href])  # resultè¿”å›æ•°å­—ï¼Œè¡¨ç¤ºå—å½±å“çš„è®°å½•æ¡æ•°ï¼Œå³ï¼Œå¯ä»¥æŸ¥çœ‹æ˜¯å¦ä¸º0,æ¥åˆ¤æ–­æœ‰æ²¡æœ‰selectåˆ°æ•°æ®è®°å½•

if result:
   print('ç½‘ç«™æœªæ›´æ–°,æ— éœ€æŠ“å–')
else:
   (æ‰§è¡Œä»£ç è¯­å¥)

self.db.commit()

-------------------------------------------------------------------

#### (é‡è¦)pythonè¯­æ³•ä¸­çš„åˆ‡ç‰‡ç”¨æ³• -> å–åˆ—è¡¨çš„åå››ä½: list1[-4:];  å–åˆ—è¡¨çš„åä¸¤ä½: list1[-2:] ; å–åˆ—è¡¨çš„å‰ä¸¤ä½: list1[:2]; å–åˆ—è¡¨çš„å‰å››ä½: list1[:4]

-------------------------------------------------------------------

selenium - Webå®¢æˆ·ç«¯éªŒè¯
å¼¹çª—ä¸­çš„ç”¨æˆ·åå’Œå¯†ç å¦‚ä½•è¾“å…¥ï¼Ÿ
-> ä¸ç”¨è¾“å…¥ï¼Œåœ¨URLåœ°å€ä¸­è¾“å…¥å°±å¯ä»¥

-------------------------------------------------------------------

selenium - Webå®¢æˆ·ç«¯éªŒè¯
å¼¹çª—ä¸­çš„ç”¨æˆ·åå’Œå¯†ç å¦‚ä½•è¾“å…¥ï¼Ÿ -> ä¸ç”¨è¾“å…¥ï¼Œåœ¨URLåœ°å€ä¸­è¾“å…¥å³å¯;

-------------------------------------------------------------------

selenium - iframeå­æ¡†æ¶(é‡è¦)

# ç‰¹ç‚¹ -> ç½‘é¡µä¸­åµŒå¥—äº†ç½‘å‹ä¹Ÿï¼Œå…ˆåˆ‡æ¢åˆ°iframeå­æ¡†æ¶ï¼Œç„¶åå†æ‰§è¡Œå…¶ä»–æ“ä½œ;
# æ–¹æ³• -> browser.switch_to.iframe(iframe_element)

ä¾‹: QQé‚®ç®±ç™»å½•


# seleniumå¸¸ç”¨æ“ä½œ
1ã€é”®ç›˜æ“ä½œ
   from selenium.webdriver.common.keys import Keys
   node.send_keys(Keys.SPACE)
   node.send_keys(Keys.CONTROL,'a')
   node.send_keys(Keys.CONTROL,'c')
   node.send_keys(Keys.CONTROL,'v')
   node.send_keys(Keys.ENTER)

2ã€é¼ æ ‡æ“ä½œ
   from selenium.webdriver import ActionChains
   mouse_action = ActionChains(browser)
   mouse_action.move_to_element(node)
   mouse_action.perform()

3ã€åˆ‡æ¢å¥æŸ„
   all_handles = browser.window_handles
   browser.switch_to.window(all_handles[1])

4ã€iframeå­æ¡†æ¶
   browser.switch_to.frame(iframe_element)

5ã€webå®¢æˆ·ç«¯éªŒè¯
url = 'http://ç”¨æˆ·å:å¯†ç @æ­£å¸¸åœ°å€'

-------------------------------------------------------------------

# execjsæ¨¡å—ä½¿ç”¨
1ã€å®‰è£…
sudo pip3 install pyexecjs

2ã€ä½¿ç”¨
with open('file.js','r') as f:
   js = f.read()

obj = execjs.compile(js)
result = obj.eval('string')

-------------------------------------------------------------------

ç™¾åº¦ç¿»è¯‘ç ´è§£æ¡ˆä¾‹
#ç›®æ ‡ -> ç ´è§£ç™¾åº¦ç¿»è¯‘æ¥å£ï¼ŒæŠ“å–ç¿»è¯‘ç»“æœæ•°æ®

#å®ç°æ­¥éª¤
1ã€F12æŠ“åŒ…ï¼Œæ‰¾åˆ°jsonçš„åœ°å€ï¼Œè§‚å¯ŸæŸ¥è¯¢å‚æ•°
     1ã€POSTåœ°å€: https://fanyi.baidu.com/v2transapi
     2ã€Formè¡¨å•æ•°æ®(å¤šæ¬¡æŠ“å–åœ¨å˜çš„å­—æ®µ)
     	from: zh
     	to: en
     	sign : 54706.276099 # è¿™ä¸ªæ˜¯å¦‚ä½•ç”Ÿæˆçš„ï¼Ÿ
     	token: a927248ae7146c842bb4a94457ca35ee # åŸºæœ¬å›ºå®šï¼Œä½†æ˜¯ä¹Ÿæƒ³åŠæ³•è·å–
     	
2ã€æŠ“å–ç›¸å…³JSæ–‡ä»¶
     1ã€å³ä¸Šè§’ - æœç´¢ - sign: - æ‰¾åˆ°å…·ä½“JSæ–‡ä»¶
       (index_c8a141d.js) - æ ¼å¼åŒ–è¾“å‡º

3ã€åœ¨JSä¸­å¯»æ‰¾signçš„ç”Ÿæˆä»£ç 
     1ã€åœ¨æ ¼å¼åŒ–è¾“å‡ºçš„JSä»£ç ä¸­æœç´¢: sign: æ‰¾åˆ°å¦‚ä¸‹JSä»£ç : sign : m(a),
     2ã€é€šè¿‡è®¾ç½®æ–­ç‚¹ï¼Œæ‰¾åˆ°m(a)å‡½æ•°çš„ä½ç½®ï¼Œå³ç”Ÿæˆsignçš„å…·ä½“å‡½æ•°
     
å…·ä½“ä»£ç å®ç°
1ã€è·å–tokenå’Œgtkçš„å€¼
  GETåœ°å€ï¼šç™¾åº¦ç¿»è¯‘é¦–é¡µå‘è¯·æ±‚ï¼Œä»å“åº”å†…å®¹è·å–;https://fanyi.baidu.com/?aldtype=16047
2ã€POSTè¯·æ±‚
  https://fanyi.baidu.com/v2transapi
  res = requests.post(URL,data=data,headers=headers)

-------------------------------------------------------------------

scrapyæ¡†æ¶
#å®šä¹‰ -> å¼‚æ­¥å¤„ç†æ¡†æ¶ï¼Œå¯é…ç½®å’Œå¯æ‰©å±•ç¨‹åº¦éå¸¸é«˜ï¼ŒPythonä¸­ä½¿ç”¨æœ€å¹¿æ³›çš„çˆ¬è™«æ¡†æ¶;

#å®‰è£… ->
   # Ubuntuå®‰è£…
   1ã€å®‰è£…ä¾èµ–åŒ…
       1ã€sudo apt-get install libffi-dev
       2ã€sudo apt-get install libssl-dev
       3ã€sudo apt-get install libxml2-dev
       4ã€sudo apt-get install python3-dev
       5ã€sudo apt-get install libxslt1-dev
       6ã€sudo apt-get install zlib1g-dev
       7ã€sudo pip3 install -I -U service_identity
       
   2ã€å®‰è£…scrapyæ¡†æ¶
       1ã€sudo pip3 install Scrapy

# Scrapyäº”å¤§ç»„ä»¶:
   1ã€å¼•æ“ ->      : æ•´ä¸ªæ¡†æ¶æ ¸å¿ƒ
   2ã€è°ƒåº¦å™¨ ->    : ç»´æŠ¤è¯·æ±‚é˜Ÿåˆ—
   3ã€çˆ¬è™«æ–‡ä»¶ ->  : æ•°æ®è§£ææå–
   4ã€ä¸‹è½½å™¨ ->    : è·å–å“åº”å¯¹è±¡
   5ã€é¡¹ç›®ç®¡é“ ->  : æ•°æ®å…¥åº“å¤„ç†


   1ã€å¼•æ“ ->  I æ•´ä¸ªæ¡†æ¶çš„æ ¸å¿ƒï¼Œæ‰€æœ‰æ•°æ®æµçš„ä¼ è¾“éƒ½è¦ç»è¿‡å¼•æ“;å¼•æ“å‘çˆ¬è™«æ–‡ä»¶ç´¢è¦ç¬¬ä¸€ä¸ªè¦çˆ¬å–çš„URLåœ°å€;
   2ã€è°ƒåº¦å™¨ -> II å¼•æ“æŠŠè·å–åˆ°çš„URLåœ°å€ä¼ ç»™è°ƒåº¦å™¨é˜Ÿåˆ—;ç„¶åå‡ºé˜Ÿåˆ—ï¼Œå†æŠŠURLä¼ ç»™å¼•æ“;
   3ã€çˆ¬è™«æ–‡ä»¶ -> V å¼•æ“æŠŠresponseä¼ ç»™çˆ¬è™«æ–‡ä»¶ï¼Œçˆ¬è™«æ–‡ä»¶è§£ææ•°æ®; VI çˆ¬è™«æ–‡ä»¶æŠŠè§£æå¥½çš„æ•°æ®å’Œç»§ç»­è·Ÿè¿›çš„URLï¼Œä¼ ç»™å¼•æ“;
   4ã€ä¸‹è½½å™¨ ->  III å¼•æ“å†æŠŠURLä¼ ç»™ä¸‹è½½å™¨; IV ä¸‹è½½å™¨æ ¹æ®URLï¼Œé€šè¿‡Internetè®¿é—®æœåŠ¡å™¨ï¼Œå°†è·å–åˆ°çš„responseè¿”å›ç»™å¼•æ“;
   5ã€é¡¹ç›®ç®¡é“ -> VII å¼•æ“å°†è§£æå¥½çš„æ•°æ®ä¼ ç»™é¡¹ç›®ç®¡é“;

-------------------------------------------------------------------

(é‡è¦ï¼)
# ä¸‹è½½å™¨ä¸­é—´ä»¶(Downloader Middlewares) : å¼•æ“ -> ä¸‹è½½å™¨ï¼ŒåŒ…è£…è¯·æ±‚(éšæœºä»£ç†ç­‰)
# èœ˜è››ä¸­é—´ä»¶(Spider Middlewares) : å¼•æ“ -> çˆ¬è™«æ–‡ä»¶ï¼Œå¯ä¿®æ”¹å“åº”å¯¹è±¡å±æ€§

# Scrapyçˆ¬è™«å·¥ä½œæµç¨‹
doesn't match either of '*.cdn.myqcloud.com',
# çˆ¬è™«é¡¹ç›®å¯åŠ¨
1ã€ç”±å¼•æ“å‘çˆ¬è™«ç¨‹åºç´¢è¦ç¬¬ä¸€ä¸ªè¦çˆ¬å–çš„URLï¼Œäº¤ç»™è°ƒåº¦å™¨å»å…¥é˜Ÿåˆ—
2ã€è°ƒåº¦å™¨å¤„ç†è¯·æ±‚åå‡ºé˜Ÿåˆ—ï¼Œé€šè¿‡ä¸‹è½½å™¨ä¸­é—´ä»¶äº¤ç»™ä¸‹è½½å™¨å»ä¸‹è½½
3ã€ä¸‹è½½å™¨å¾—åˆ°å“åº”å¯¹è±¡åï¼Œé€šè¿‡èœ˜è››ä¸­é—´ä»¶äº¤ç»™çˆ¬è™«ç¨‹åº
4ã€çˆ¬è™«ç¨‹åºè¿›è¡Œæ•°æ®æå–:
   1ã€æ•°æ®äº¤ç»™ç®¡é“æ–‡ä»¶å»å…¥åº“å¤„ç†;
   2ã€å¯¹äºéœ€è¦ç»§ç»­è·Ÿè¿›çš„URLï¼Œå†æ¬¡äº¤ç»™è°ƒåº¦å™¨å…¥é˜Ÿåˆ—ï¼Œä¾æ¬¡å¾ªç¯;

-------------------------------------------------------------------

# Scrapyå¸¸ç”¨å‘½ä»¤
1ã€åˆ›å»ºçˆ¬è™«é¡¹ç›®

scrapy startproject é¡¹ç›®å

2ã€åˆ›å»ºçˆ¬è™«æ–‡ä»¶

scrapy  genspider çˆ¬è™«å åŸŸå

3ã€è¿è¡Œçˆ¬è™«

scrapy crawl çˆ¬è™«å

-------------------------------------------------------------------

å…¨å±€é…ç½®æ–‡ä»¶settings.pyè¯¦è§£

1.å®šä¹‰User-Agent

USER_AGENT= 'Mozilla/5.0'

2.æ˜¯å¦éµå¾ªrobotsåè®®ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºFalse

ROBOTSTXT_OBEY = False

3.æœ€å¤§å¹¶å‘é‡ï¼Œé»˜è®¤ä¸º16

CONCURRENT_REQUESTS = 32

4.ä¸‹è½½å»¶è¿Ÿæ—¶é—´ï¼Œæ§åˆ¶çˆ¬å–æ—¶é—´

#### DOWNLOAD_DELAY = 1ï¼ˆé‡è¦ï¼ï¼ï¼‰

5.è¯·æ±‚å¤´ï¼Œæ­¤å¤„ä¹Ÿå¯ä»¥æ·»åŠ User-Agent

-------------------------------------------------------------------

åˆ›å»ºçˆ¬è™«é¡¹ç›®æ­¥éª¤
1ã€æ–°å»ºé¡¹ç›®: scrapy startproject é¡¹ç›®å
2ã€cd é¡¹ç›®æ–‡ä»¶å¤¹
3ã€æ–°å»ºçˆ¬è™«æ–‡ä»¶ : scrapy genspider æ–‡ä»¶å åŸŸå
4ã€æ˜ç¡®ç›®æ ‡(items.py)
5ã€å†™çˆ¬è™«ç¨‹åº(æ–‡ä»¶å.py)
6ã€ç®¡é“æ–‡ä»¶(pipelines.py)
7ã€å…¨å±€é…ç½®(settings.py)
8ã€è¿è¡Œçˆ¬è™«:scrapy crawl çˆ¬è™«å

-------------------------------------------------------------------

Tips:linuxç³»ç»Ÿä¸­ï¼ŒæŸ¥çœ‹æ–‡ä»¶å­˜å‚¨ç»“æ„è·¯å¾„çš„å‘½ä»¤: tree æ–‡ä»¶å

-------------------------------------------------------------------

ScrapyçŸ¥è¯†ç‚¹æ±‡æ€»
## èŠ‚ç‚¹å¯¹è±¡.xpath('')
1ã€åˆ—è¡¨ï¼Œå…ƒç´ ä¸ºé€‰æ‹©å…¶['<selector data='A'>]
2ã€åˆ—è¡¨.extract():åºåˆ—åŒ–åˆ—è¡¨ä¸­æ‰€æœ‰é€‰æ‹©å…¶ä¸ºUnicodeå­—ç¬¦ä¸²['A','B','C']
3ã€åˆ—è¡¨.extract_first()æˆ–è€…get():è·å–åˆ—è¡¨ä¸­ç¬¬1ä¸ªåºåˆ—åŒ–çš„å…ƒç´ (å­—ç¬¦ä¸²)

## pipelines.pyä¸­å¿…é¡»æœ‰1ä¸ªå‡½æ•°å«process_item
def process_item(self,item,spider):
    return item
## å¿…é¡»è¿”å›itemï¼Œæ­¤è¿”å›å€¼ä¼šä¼ ç»™ä¸‹ä¸€ä¸ªç®¡é“çš„æ­¤å‡½æ•°ç»§ç»­å¤„ç†

-------------------------------------------------------------------

æ—¥å¿—çš„å˜é‡åŠæ—¥å¿—çš„çº§åˆ«(settings.py)

## æ—¥å¿—ç›¸å…³å˜é‡
LOG_LEVEL = ''
LOG_FILE = 'æ–‡ä»¶å.log' -> è¾“å‡ºlogæ–‡ä»¶ï¼Œè®°å½•logä¿¡æ¯

# æ—¥å¿—çº§åˆ«

5 CRITICAL : ä¸¥é‡é”™è¯¯
4 ERROR : æ™®é€šé”™è¯¯
3 WARNING : è­¦å‘Š
2 INFO : ä¸€èˆ¬ä¿¡æ¯
1 DEBUG : è°ƒè¯•ä¿¡æ¯

## æ³¨æ„: åªæ˜¾ç¤ºå½“å‰çº§åˆ«çš„æ—¥å¿—å’Œæ¯”å½“å‰çº§åˆ«æ—¥å¿—æ›´ä¸¥é‡çš„

-------------------------------------------------------------------

ç®¡é“æ–‡ä»¶ä½¿ç”¨
1ã€çˆ¬è™«æ–‡ä»¶ä¸­ä¸ºitems.pyä¸­ç±»åšå®ä¾‹åŒ–ï¼Œç”¨çˆ¬ä¸‹æ¥çš„æ•°æ®ç»™å¯¹è±¡èµ‹å€¼
	from ..items import MaoyanItem
	item = MaoyanItem()
	item['æ•°æ®å˜é‡å'] = æ•°æ®å˜é‡
2ã€ç®¡é“æ–‡ä»¶(pipelines.py)
3ã€å¼€å¯ç®¡é“(settings.py)
	ITEM_PIPELINES = {'é¡¹ç›®ç›®å½•å.pipelines.ç±»å':ä¼˜å…ˆçº§}

-------------------------------------------------------------------

(é‡è¦ï¼)Scrapyä¸­ï¼Œå°†è¾“å‡ºä¿¡æ¯ä¿å­˜ä¸ºcsvã€jsonæ–‡ä»¶

å‘½ä»¤æ ¼å¼

scrapy crawl maoyan -o maoyan.csv
scrapy crawl maoyan -o maoyan.json

## settings.pyä¸­è®¾ç½®å¯¼å‡ºç¼–ç 
FEED_EXPORT_ENCODING = 'utf-8'

-------------------------------------------------------------------

Scrapyå·¥ä½œæµç¨‹
1ã€Engineå‘Spiderç´¢è¦URLï¼Œäº¤ç»™ Schedulerå…¥é˜Ÿåˆ—
2ã€Schedulerå¤„ç†åå‡ºé˜Ÿåˆ—ï¼Œé€šè¿‡Downloader Middlewaresäº¤ç»™Downloaderå»ä¸‹è½½
3ã€Downloaderå¾—åˆ°å“åº”åï¼Œé€šè¿‡Spider Middlewaresäº¤ç»™Spider
4ã€Spideræ•°æ®æå–:
    1ã€æ•°æ®äº¤ç»™Pipelineå¤„ç†
    2ã€éœ€è¦è·Ÿè¿›URLï¼Œç»§ç»­äº¤ç»™Schedulerå…¥é˜Ÿåˆ—ï¼Œä¾æ¬¡å¾ªç¯
    
-------------------------------------------------------------------

å¸¸ç”¨å‘½ä»¤
#åˆ›å»ºçˆ¬è™«é¡¹ç›®
scrapy startproject é¡¹ç›®å

#åˆ›å»ºçˆ¬è™«æ–‡ä»¶
cd é¡¹ç›®æ–‡ä»¶å¤¹
scrapy genspider çˆ¬è™«å åŸŸå

#è¿è¡Œçˆ¬è™«
scrapy crawl çˆ¬è™«å

-------------------------------------------------------------------

Scrapyä½¿ç”¨æµç¨‹
1.scrapy startproject Tencent
2.cd Tencent
3.scrapy genspider tencent tencent.com
4.items.py(å®šä¹‰çˆ¬å–æ•°æ®ç»“æ„)
	import scrapy
	class TencentItem(scrapy.Item):
		job_name=scrapy.Field()
5.tencent.py(å†™çˆ¬è™«æ–‡ä»¶)
	import scrapy
	class TencentSpider(scrapy.Spider):
		name = 'tencent'
		allowed_domains = ['tencent.com']
		start_urls = ['http://tencent.com/']
		def parse(self,response):
			pass
		
6.pipelines.py(æ•°æ®å¤„ç†)
	class TencentPipeline(Object):
		def process_item(self,item,spider):
			return item
7.settings.py(å…¨å±€é…ç½®)
	ROBOTSTXT_OBEY = False
	DEFAULT_REQUEST_HEADERS = {}
	ITEM_PIPELINES = {'':200}
8.ç»ˆç«¯:scrapy crawl tencent

-------------------------------------------------------------------

å“åº”å¯¹è±¡å±æ€§åŠæ–¹æ³•
## å±æ€§
1ã€response.text: è·å–å“åº”å†…å®¹ - å­—ç¬¦ä¸²
2ã€response.body: è·å–bytesæ•°æ®ç±»å‹
3ã€response.xpath('')

## response.xpath('')è°ƒç”¨æ–¹æ³•
1ã€ç»“æœ:åˆ—è¡¨ï¼Œå…ƒç´ ä¸ºé€‰æ‹©å™¨å¯¹è±¡
	# <selector xpath='//article' data=''>
2ã€.extract():æå–æ–‡æœ¬å†…å®¹ï¼Œå°†åˆ—è¡¨ä¸­æ‰€æœ‰å…ƒç´ åºåˆ—åŒ–ä¸ºUnicodeå­—ç¬¦ä¸²
3ã€.extract_first():æå–åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªæ–‡æœ¬å†…å®¹
4ã€.get():æå–åˆ—è¡¨ä¸­ç¬¬ä¸€ä¸ªæ–‡æœ¬å†…å®¹

-------------------------------------------------------------------

çˆ¬è™«é¡¹ç›®å¯åŠ¨æ–¹å¼

æ–¹å¼ä¸€
1ã€ä»çˆ¬è™«æ–‡ä»¶(spider)çš„start_urlså˜é‡ä¸­éå†URLåœ°å€ï¼ŒæŠŠä¸‹è½½å™¨è¿”å›çš„å“åº”å¯¹è±¡(response)äº¤ç»™çˆ¬è™«æ–‡ä»¶çš„parse()å‡½æ•°å¤„ç†
2ã€# start_urls = ['http://www.baidu.com/']

æ–¹å¼äºŒ
é‡å†™start_requests()æ–¹æ³•ï¼Œä»æ­¤æ–¹æ³•ä¸­è·å–URLï¼Œäº¤ç»™æŒ‡å®šçš„callbackè§£æå‡½æ•°å¤„ç†

1ã€å»æ‰start_urlså˜é‡
2ã€def start_requests(self):
	# ç”Ÿæˆè¦çˆ¬å–çš„URLåœ°å€ï¼Œåˆ©ç”¨scrapy.Request()æ–¹æ³•äº¤ç»™è°ƒåº¦å™¨
	
-------------------------------------------------------------------

æ—¥å¿—çº§åˆ«

DEBUG < INFO < WARNING < ERROR < CRITICAL

-------------------------------------------------------------------

æ•°æ®æŒä¹…åŒ–å­˜å‚¨(MySQLã€MongoDB)
1ã€åœ¨ settings.pyä¸­å®šä¹‰ç›¸å…³å˜é‡
2ã€pipelines.pyä¸­æ–°å»ºç®¡é“ç±»ï¼Œå¹¶å¯¼å…¥settingsæ¨¡å—
	def open_spider(self,spider):
		# çˆ¬è™«å¼€å§‹æ‰§è¡Œ1æ¬¡ï¼Œç”¨äºæ•°æ®åº“è¿æ¥
		
	def process_item(self,item,spider):
		# ç”¨äºå¤„ç†æŠ“å–çš„itemæ•°æ®
		
	def close_spider(self,spider):
		# çˆ¬è™«ç»“æŸæ—¶æ‰§è¡Œ1æ¬¡ï¼Œç”¨äºæ–­å¼€æ•°æ®åº“è¿æ¥
3ã€settings.pyä¸­æ·»åŠ æ­¤ç®¡é“
	ITEM_PIPELINES ={'':200}
	
## æ³¨æ„: process_item() å‡½æ•°ä¸­ä¸€å®šè¦return item

-------------------------------------------------------------------

ä¿å­˜ä¸ºcsvã€jsonæ–‡ä»¶
## å‘½ä»¤æ ¼å¼
scrapy crawl maoyan -o maoyan.csv
scrapy crawl maoyan -o maoyan.json
## settings.py FEED_EXPORT_ENCODING = 'utf-8'

-------------------------------------------------------------------

settings.pyå¸¸ç”¨å˜é‡
#1ã€è®¾ç½®æ—¥å¿—çº§åˆ«
LOG_LEVEL = ''
#2ã€ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶(ä¸åœ¨ç»ˆç«¯ è¾“å‡º)
LOG_FILE = ''
#3ã€è®¾ç½®æ•°æ®å¯¼å‡ºç¼–ç (ä¸»è¦é’ˆå¯¹äºjsonæ–‡ä»¶)
FEED_EXPORT_ENCODING = ''
#4ã€éç»“æ„åŒ–æ•°æ®å­˜å‚¨è·¯å¾„
IMAGES_STROE = 'è·¯å¾„'
#5ã€è®¾ç½®User-Agent
USER_AGENT = ''
#6ã€è®¾ç½®æœ€å¤§å¹¶å‘æ•°(é»˜è®¤ä¸º16)
CONCURRENT_REQUESTS = 32
#7ã€ä¸‹è½½å»¶è¿Ÿæ—¶é—´(æ¯éš”å¤šé•¿æ—¶é—´è¯·æ±‚ä¸€ä¸ªç½‘é¡µ)
###### DOWNLOAD_DELAY ä¼šå½±å“ CONCURRENT_REQUESTS,ä¸èƒ½ä½¿å¹¶å‘æ˜¾ç°â€™
###### æœ‰CONCURRENT_REQUESTS,æ²¡æœ‰DOWNLOAD_DELAY: æœåŠ¡å™¨ä¼šåœ¨åŒä¸€æ—¶é—´æ”¶åˆ°å¤§é‡çš„è¯·æ±‚
###### æœ‰CONCURRENT_REQUESTS,æœ‰DOWNLOAD_DELAYæ—¶ï¼ŒæœåŠ¡å™¨ä¸ä¼šåœ¨åŒä¸€æ—¶é—´æ”¶åˆ°å¤§é‡çš„è¯·æ±‚
DOWNLOAD_DELAY = 3
#8ã€è¯·æ±‚å¤´
DEFAULT_REQUEST_HEADERS = {}
#9ã€æ·»åŠ é¡¹ç›®ç®¡é“
ITEM_PiPELINES = {}
#10ã€æ·»åŠ ä¸‹è½½å™¨ä¸­é—´ä»¶
DOWNLOADER_MIDDLEWARES = {}

-------------------------------------------------------------------

éç»“æ„åŒ–æ•°æ®æŠ“å–
1ã€spider
	yield item['é“¾æ¥']
2ã€pipelines.py
	from scrapy.pipelines.images import ImagesPipeline
	import scrapy
	class TestPipeline(ImagesPipeline):
		def get_media_requests(self,item,info):
			yield scrapy.Request(url=item['url'],meta={'item':item['name']})
		
		def file_path(self,request,response=None,info=None):
			name = request.meta['item']
			filename = name
			return filename
3ã€settings.py
	IMAGES_STORE = 'D:\\Spider\\images'			


-------------------------------------------------------------------

scrapy.Request()å‚æ•°
1ã€url
2ã€callback
3ã€meta:ä¼ é€’æ•°æ®ï¼Œå®šä¹‰ä»£ç†
	
-------------------------------------------------------------------

scrapy.Request()

#### å‚æ•°

1ã€url
2ã€callback
3ã€headers
4ã€meta : ä¼ é€’æ•°æ®ï¼Œå®šä¹‰ä»£ç†
5ã€dont_filter : æ˜¯å¦å¿½ç•¥åŸŸç»„é™åˆ¶ - é»˜è®¤Falseï¼Œæ£€æŸ¥allowed_domains['']

#### requestå±æ€§
1ã€request.url
2ã€request.headers
3ã€request.meta

-------------------------------------------------------------------

itemå¯¹è±¡åˆ°åº•è¯¥åœ¨ä½•å¤„åˆ›å»ºï¼Ÿ
1ã€ä¸€çº§é¡µé¢: éƒ½å¯ä»¥ï¼Œå»ºè®®åœ¨forå¾ªç¯å¤–
2ã€å¤§äºäºŒçº§é¡µé¢: forå¾ªç¯å†…

-------------------------------------------------------------------
scrapy shellçš„ä½¿ç”¨

$åŸºæœ¬ä½¿ç”¨
#scrapy shell URLåœ°å€
1ã€request.url        : è¯·æ±‚URLåœ°å€
2ã€request.headers    : è¯·æ±‚å¤´(å­—å…¸)
3ã€request.meta       : itemæ•°æ®ä¼ é€’ï¼Œå®šä¹‰ä»£ç†(å­—å…¸)

4ã€response.text      : å­—ç¬¦ä¸²
5ã€response.body      : bytes
6ã€response.xpath('') 

-------------------------------------------------------------------

#scrapy.Request()å‚æ•°  -> å…¥é˜Ÿåˆ—
1ã€url
2ã€callback
3ã€headers
4ã€meta : ä¼ é€’æ•°æ®ï¼Œå®šä¹‰ä»£ç†
5ã€dont_filter : æ˜¯å¦å¿½ç•¥åŸŸç»„é™åˆ¶
   é»˜è®¤Falseï¼Œæ£€æŸ¥allowed_domains['']

eg: yield scrapy.Request(url=url,dont_filter=True)

-------------------------------------------------------------------

è®¾ç½®ä¸­é—´ä»¶(éšæœºUser-Agent)

å°‘é‡User-Agentåˆ‡æ¢

#æ–¹æ³•ä¸€
#settings.py
USER-AGENT = ''
DEFAULT_REQUEST_HEADERS = {}

#æ–¹æ³•äºŒ
#spider
yield scrapy.Request(url,callback=å‡½æ•°å,headers={})

-------------------------------------------------------------------

å¤§é‡User-Agentåˆ‡æ¢(ä¸­é—´ä»¶)
#middlewares.pyè®¾ç½®ä¸­é—´ä»¶
1ã€è·å–User-Agent
	# æ–¹æ³•1: æ–°å»ºuseragents.pyï¼Œå­˜æ”¾å¤§é‡User-Agentï¼Œrandomæ¨¡å—éšæœºåˆ‡æ¢  -> headers = {'User-Agent': random.choice(ua_list)}
	# æ–¹æ³•2: å®‰è£…fake_useragentæ¨¡å—(sudo pip3 install fake_useragent)
â€‹		from fake_useragent import UserAgent
â€‹		ua_obj = UserAgent()
â€‹		ua = ua_obj.random
2ã€middlewares.pyæ–°å»ºä¸­é—´ä»¶ç±»
â€‹	class RandomUseragentMiddleware(object):
â€‹		def process_request(self,request,spider):
â€‹			ua = UserAgent()
â€‹			request.headers['User-Agent'] = ua.random
3ã€settings.pyæ·»åŠ æ­¤ä¸‹è½½å™¨ä¸­é—´ä»¶
â€‹	DOWNLOADER_MIDDLERWARES = {'':ä¼˜å…ˆçº§}

-------------------------------------------------------------------

### æ·»åŠ ä¸­é—´ä»¶ - éšæœºä»£ç†IP

from .proxies import proxy_list
import random
class MiddleRandomProxyMiddleware:
    def process_request(self,request,spider):
        proxy = random.choice(proxy_list)
        print(proxy)
        # request.metaå±æ€§: ç±»å‹ä¸ºå­—å…¸
        request.meta['proxy'] = proxy

    # æ•è·å¼‚å¸¸
    def process_exception(self,request,exception,spider):
        # ä¸€æ—¦æ•è·åˆ°å¼‚å¸¸ï¼ŒæŠŠrequesté‡æ–°äº¤ç»™ä¸­é—´ä»¶
        return  request

-> éšæœºä»£ç†IPå‚è€ƒä¸Šè¿°ä»£ç ï¼Œç‹¬äº«IPä¸ç”¨è€ƒè™‘ä¸Šè¿°ä»£ç 

-------------------------------------------------------------------

FiddleræŠ“åŒ…å·¥å…·
#é…ç½®Fiddler
#æ·»åŠ è¯ä¹¦ä¿¡ä»»
1ã€Tools - Options - HTTPS
	å‹¾é€‰Decrypt Https Traffic åå¼¹å‡ºçª—å£ï¼Œä¸€è·¯ç¡®è®¤
	
#è®¾ç½®åªæŠ“å–æµè§ˆå™¨çš„æ•°æ®åŒ…
2ã€...from browsers only

#è®¾ç½®ç›‘å¬ç«¯å£(é»˜è®¤ä¸º8888)
3ã€Tools - Options - Connections

#é…ç½®å®Œæˆåé‡å¯Fiddler(é‡è¦)
4ã€å…³é—­Fiddlerï¼Œå†æ‰“å¼€Fiddler

-------------------------------------------------------------------

#é…ç½®æµè§ˆå™¨ä»£ç†
1ã€å®‰è£…Proxy SwitchyOmegaæ’ä»¶
2ã€æµè§ˆå™¨å³ä¸Šè§’:SwitchyOmega->é€‰é¡¹->æ–°å»ºæƒ…æ™¯æ¨¡å¼->AID1904(åå­—)->åˆ›å»º
	è¾“å…¥  : HTTP:// 127.0.0.1 8888
	ç‚¹å‡»  : åº”ç”¨é€‰é¡¹
3ã€ç‚¹å‡»å³ä¸Šè§’SwitchyOmegaå¯åˆ‡æ¢ä»£ç†

-------------------------------------------------------------------

#Fiddlerå¸¸ç”¨èœå• 
1ã€Inspector : æŸ¥çœ‹æ•°æ®åŒ…è¯¦ç»†å†…å®¹
	æ•´ä½“åˆ†ä¸ºè¯·æ±‚å’Œå“åº”ä¸¤éƒ¨åˆ†
2ã€å¸¸ç”¨èœå•
	Headers: è¯·æ±‚å¤´ä¿¡æ¯
	webForms
		#1ã€POSTè¯·æ±‚Formè¡¨å•æ•°æ®: <body>
		#2ã€GETè¯·æ±‚æŸ¥è¯¢å‚æ•°: <QueryString>
	Raw:å°†æ•´ä¸ªè¯·æ±‚æ˜¾ç¤ºä¸ºçº¯æ–‡æœ¬
	
	
-------------------------------------------------------------------

åˆ†å¸ƒå¼çˆ¬è™«
åˆ†å¸ƒå¼çˆ¬è™«ä»‹ç»

*åŸç†
å¤šå°ä¸»æœºå…±äº«1ä¸ªçˆ¬å–é˜Ÿåˆ—

*å®ç°
é‡å†™scrapyè°ƒåº¦å™¨(scrapy_redisæ¨¡å—)

*ä¸ºä»€ä¹ˆä½¿ç”¨redis
1ã€RedisåŸºäºå†…å­˜ï¼Œé€Ÿåº¦å¿«;
2ã€Rediséå…³ç³»å‹æ•°æ®åº“ï¼ŒRedisä¸­é›†åˆï¼Œå­˜å‚¨æ¯ä¸ªrequestçš„æŒ‡çº¹;
3ã€scrapy_rediså®‰è£…
	sudo  pip3 install scrapy_redis

-------------------------------------------------------------------

scrapy_redisè¯¦è§£
* GitHubåœ°å€
* settings.pyè¯´æ˜
#é‡æ–°æŒ‡å®šè°ƒåº¦å™¨:å¯ç”¨Redisè°ƒåº¦å­˜å‚¨è¯·æ±‚é˜Ÿåˆ—
SCHEDULER = "scrapy_redis.scheduler.Scheduler"

#é‡æ–°æŒ‡å®šå»é‡æœºåˆ¶:ç¡®ä¿æ‰€æœ‰çš„çˆ¬è™«é€šè¿‡Rediså»é‡
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"

#ä¸æ¸…é™¤Redisé˜Ÿåˆ—:æš‚åœ/æ¢å¤/æ–­ç”µç»­çˆ¬
SCHEDULER_PERSIST = True  # Trueè¡¨ç¤ºä¸æ¸…é™¤

#ä¼˜å…ˆçº§é˜Ÿåˆ—(é»˜è®¤)
SCHEDULER_QUEUE_CLASS='scrapy_redis.queue.PriorityQueue'

#å¯é€‰ç”¨çš„å…¶ä»–é˜Ÿåˆ—
#å…ˆè¿›å…ˆå‡ºé˜Ÿåˆ—
SCHEDULER_QUEUE_CLASS= 'scrapy_redis.queue.FifoQueue'

#åè¿›å…ˆå‡ºé˜Ÿåˆ—
SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'

#redisç®¡é“
ITEM_PIPELINES ={
	'scrapy_redis.pipelines.RedisPipeline':300
}

#æŒ‡å®šè¿æ¥åˆ°redisæ—¶ä½¿ç”¨çš„ç«¯å£å’Œåœ°å€
REDIS_HOST = 'localhost'
REDIS_PORT = 6379

-------------------------------------------------------------------

1ã€æ­£å¸¸é¡¹ç›®æ•°æ®æŠ“å–(éåˆ†å¸ƒå¼)
2ã€æ”¹å†™ä¸ºåˆ†å¸ƒå¼(åŒæ—¶å­˜å…¥redis)
	1ã€settings.py
		#ä½¿ç”¨scrapy_redisçš„è°ƒåº¦å™¨
		SCHEDULER = "scrapy_redis.scheduler.Scheduler"
		
		#ä½¿ç”¨scrapy_redisçš„å»é‡æœºåˆ¶
		DUPEFILTER_CLASS="scrapy_redis.dupefilter.RFPDupeFilter"
		
		#æ˜¯å¦æ¸…é™¤è¯·æ±‚æŒ‡çº¹,True:ä¸æ¸…é™¤  |   False:æ¸…é™¤
		SCHEDULER_PERSIST = True
		
		#åœ¨ITEM_PIPELINESä¸­æ·»åŠ redisç®¡é“
		'scrapy_redis.pipelines.RedisPipeline':200
		
		#å®šä¹‰redisä¸»æœºåœ°å€å’Œç«¯å£å·
		REDIS_HOST = '111.111.111.111'
		REDIS_PORT = 6379

-------------------------------------------------------------------

æ”¹å†™ä¸ºåˆ†å¸ƒå¼(åŒæ—¶å­˜å…¥mysql)
#ä¿®æ”¹ç®¡é“
ITEM_PIPELINES = {
	'Tencent.pipelines.TencentPipeline':300,
	# 'scrapy_redis.pipelines.RedisPipeline':200,
	'Tencent.pipelines.TencentMysqlPipeline':200
}

-------------------------------------------------------------------

æ¸…é™¤redisæ•°æ®åº“ -> flushdb

ä»£ç æ‹·è´ä¸€ä»½åˆ°åˆ†å¸ƒå¼ä¸­å…¶ä»–æœºå™¨ï¼Œä¸¤å°æˆ–å¤šå°æœºå™¨åŒæ—¶æ‰§è¡Œæ­¤ä»£ç 

-------------------------------------------------------------------
# è¿œç¨‹è¿æ¥MySQLè®¾ç½®

1ã€é…ç½®æ–‡ä»¶ - å…è®¸è¿œç¨‹è¿æ¥
sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf

### bind-address = 127.0.0.1  -- æŠŠæ­¤è¡Œæ³¨é‡Š

2ã€æ·»åŠ æˆæƒç”¨æˆ· - mysql -uroot -proot
mysql> grant all privileges on *.* to 'ç”¨æˆ·å'@'%' identified by 'å¯†ç ' with grant option;
mysql> flush privileges;

3ã€é‡å¯MySQLæœåŠ¡
sudo /etc/init.d/mysql restart

-------------------------------------------------------------------

è…¾è®¯æ‹›è˜åˆ†å¸ƒå¼æ”¹å†™ - æ–¹æ³•äºŒ
ä½¿ç”¨redis_keyæ”¹å†™

#ç¬¬ä¸€æ­¥:settings.pyå’Œç¬¬ä¸€ç§å†™æ³•ä¸€è‡´
settings.pyå’Œä¸Šé¢åˆ†å¸ƒå¼ä»£ç ä¸€è‡´

#ç¬¬äºŒæ­¥:çˆ¬è™«æ–‡ä»¶ - tencent.pyä¿®æ”¹
from scrapy_redis.spiders import RedisSpider

class TencentSpider(RedisSpider):
	#1.å»æ‰start_urls
	#2.å®šä¹‰redis_key
	redis_key = 'tencent:spider'
	def parse(self,response):
		pass
		
#ç¬¬ä¸‰æ­¥:æŠŠä»£ç å¤åˆ¶åˆ°æ‰€æœ‰çš„çˆ¬è™«æœåŠ¡å™¨ï¼Œå¹¶å¯åŠ¨é¡¹ç›®
#ç¬¬å››æ­¥:åˆ°rediså‘½ä»¤è¡Œï¼Œæ‰§è¡ŒLPUSHå‘½ä»¤å‹å…¥ç¬¬ä¸€ä¸ªè¦çˆ¬å–çš„URLåœ°å€

åˆ†å¸ƒå¼æ€»ç»“: -> å¤šå°ä¸»æœºå…±äº«ä¸€ä¸ªçˆ¬å–é˜Ÿåˆ—
å¦‚ä½•å®ç°? -> é€šè¿‡é‡å†™scrapyè°ƒåº¦å™¨

-------------------------------------------------------------------

scrapy - postè¯·æ±‚

### æ–¹æ³• + å‚æ•°
scrapy.FormRequest(
	url=posturl,
	formdata=formdata,
	callback=self.parse
)

# æœ‰é“ç¿»è¯‘æ¡ˆä¾‹å®ç°
1ã€åˆ›å»ºé¡¹ç›®+çˆ¬è™«æ–‡ä»¶
scrapy startproject Youdao
cd Youdao
scrapy genspider youdao fanyi.youdao.com

2ã€items.py
result = scrapy.Field()

3ã€youdao.py

æ¸©é¦¨æç¤ºï¼šå†™ä»£ç å…ˆå†™ç»“æ„ï¼Œå…·ä½“çš„å®ç°å¯ä»¥ä¹‹åå†è¡¥

-------------------------------------------------------------------

scrapyæ·»åŠ cookieçš„ä¸‰ç§æ–¹å¼
#1ã€ä¿®æ”¹ settings.pyæ–‡ä»¶
1ã€COOKIE_ENABLED = False å–æ¶ˆæ³¨é‡Š
2ã€DEFAULT_REQUEST_HEADERS = {}  æ·»åŠ Cookie

#2ã€DownloadMiddleware
COOKIES_ENABLED = True
def process_request(self,request,spider):
	request.cookies={}
	
#3ã€çˆ¬è™«æ–‡ä»¶
COOKIES_ENABLED = True
def start_requests(self):
	yield scrapy.FormRequest(url=url,cookies ={},callback=xxx)
	
------------------------------------------------------------------

æœºå™¨è§†è§‰ä¸tesseract

ä½œç”¨â€”â€”> å¤„ç†å›¾å½¢éªŒè¯ç 

ä¸‰ä¸ªé‡è¦æ¦‚å¿µ

OCR
#å®šä¹‰ -> OCR: å…‰å­¦å­—ç¬¦è¯†åˆ«(Optical character Recognition)
#åŸç† -> é€šè¿‡æ‰«æç­‰å…‰å­¦è¾“å…¥æ–¹å¼å°†å„ç§ç¥¨æ®ã€æŠ¥åˆŠã€ä¹¦ç±ã€æ–‡ç¨¿åŠå…¶å®ƒå°åˆ·å“çš„æ–‡å­—è½¬åŒ–ä¸ºå›¾åƒä¿¡æ¯ï¼Œå†åˆ©ç”¨æ–‡å­—è¯†åˆ«æŠ€æœ¯å°†å›¾åƒä¿¡æ¯è½¬åŒ–ä¸ºç”µå­æ–‡æœ¬

tesseract-ocr
OCRçš„ä¸€ä¸ªåº•å±‚è¯†åˆ«åº“(ä¸æ˜¯æ¨¡å—ï¼Œä¸èƒ½å¯¼å…¥)
# Googleç»´æŠ¤çš„å¼€æºOCRè¯†åˆ«åº“

pytesseract
Pythonæ¨¡å—ï¼Œå¯è°ƒç”¨åº•å±‚è¯†åˆ«åº“
# å¯¹tesseract-ocråšçš„ä¸€å±‚Python APIå°è£…

Ubuntuå®‰è£…tesseract-ocr
sudo apt-get install tesseract-ocr
	
æœç´¢'tessdata'
sudo find / -name 'tessdata'

è¿›å…¥ç®¡ç†å‘˜æ¨¡å¼
sudo -i
é€€å‡ºç®¡ç†å‘˜æ¨¡å¼
exit

å®‰è£…pytesseract
å®‰è£…-> sudo pip3 install pytesseract


#çˆ¬å–ç½‘ç«™æ€è·¯(éªŒè¯ç )
1ã€è·å–éªŒè¯ç å›¾ç‰‡
2ã€ä½¿ç”¨PILåº“æ‰“å¼€å›¾ç‰‡
3ã€ä½¿ç”¨pytesseractå°†å›¾ç‰‡ä¸­éªŒè¯ç è¯†åˆ«å¹¶è½¬åŒ–ä¸ºå­—ç¬¦ä¸²
4ã€å°†å­—ç¬¦ä¸²å‘é€åˆ°éªŒè¯ç æ¡†ä¸­æˆ–è€…æŸä¸ªURLåœ°å€

------------------------------------------------------------------

åœ¨çº¿æ‰“ç å¹³å°

ä¸ºä»€ä¹ˆä½¿ç”¨åœ¨çº¿æ‰“ç å¹³å°?
tesseract-ocrè¯†åˆ«ç‡å¾ˆä½ï¼Œæ–‡å­—å˜å½¢ã€å¹²æ‰°ï¼Œå¯¼è‡´æ— æ³•è¯†åˆ«éªŒè¯ç 

äº‘æ‰“ç å¹³å°ä½¿ç”¨æ­¥éª¤
1ã€ä¸‹è½½å¹¶æŸ¥çœ‹æ¥å£æ–‡æ¡£
2ã€è°ƒæ•´æ¥å£æ–‡æ¡£ï¼Œè°ƒæ•´ä»£ç å¹¶æ¥å…¥ç¨‹åºæµ‹è¯•
3ã€çœŸæ­£æ¥å…¥ç¨‹åºï¼Œåœ¨çº¿è¯†åˆ«åè·å–ç»“æœå¹¶ä½¿ç”¨


------------------------------------------------------------------

md5åŠ å¯†æ­¥éª¤:
1ã€å¯¼å…¥md5æ¨¡å— -> from hashlib import md5
2ã€åˆ›å»ºmd5å¯¹è±¡ -> s = md5()
3ã€åˆ›å»ºéœ€è¦åŠ å¯†çš„å­—ç¬¦ä¸²å¯¹è±¡ -> string = 'fanyideskweb'
4ã€å°†å­—ç¬¦ä¸²å¯¹è±¡ç¼–ç åï¼Œåœ¨md5å¯¹è±¡ä¸­è¿›è¡Œæ›´æ–° -> s.update(string.encode())
5ã€æœ€åé€šè¿‡md5å¯¹è±¡ï¼Œç”ŸæˆåŠ å¯†å¯†æ–‡  -> sign = s.hexdigest()

------------------------------------------------------------------

FiddleræŠ“åŒ…å·¥å…·
* é…ç½®Fiddler

### æ·»åŠ è¯ä¹¦ä¿¡ä»»

1ã€Tools - Options - HTTPS
	å‹¾é€‰ Decrypt Https Traffic åå¼¹å‡ºçª—å£ï¼Œä¸€è·¯ç¡®è®¤
	

### è®¾ç½®åªæŠ“å–æµè§ˆå™¨çš„æ•°æ®åŒ…
2ã€...from browsers only

### è®¾ç½®ç›‘å¬ç«¯å£(é»˜è®¤ä¸º8888)
3ã€Tools - Options - Connections

### é…ç½®å®Œæˆåé‡å¯Fiddler(é‡è¦)
4ã€å…³é—­Fiddlerï¼Œå†æ‰“å¼€Fiddler

* é…ç½®æµè§ˆå™¨ä»£ç†
1ã€å®‰è£…Proxy SwitchOmegaæ’ä»¶
2ã€æµè§ˆå™¨å³ä¸Šè§’:SwitchOmega -> é€‰é¡¹ -> æ–°å»ºæƒ…æ™¯æ¨¡å¼ -> AID1901(åå­—) -> åˆ›å»º
	è¾“å…¥ : HTTP:// 127.0.0.1 8888
	ç‚¹å‡» : åº”ç”¨é€‰é¡¹
3ã€ç‚¹å‡»å³ä¸Šè§’SwitchyOmegaå¯åˆ‡æ¢ä»£ç†
------------------------------------------------------------------
* Fiddlerå¸¸ç”¨èœå•
	1ã€Inspector : æŸ¥çœ‹æ•°æ®åŒ…è¯¦ç»†å†…å®¹
	æ•´ä½“åˆ†ä¸ºè¯·æ±‚å’Œå“åº”ä¸¤éƒ¨åˆ†

2ã€å¸¸ç”¨èœå•
	Headers : è¯·æ±‚å¤´ä¿¡æ¯
	WebForms : POSTè¯·æ±‚Formè¡¨å•æ•°æ® : <body>
	GETè¯·æ±‚æŸ¥è¯¢å‚æ•° : <QueryString>
	Raw
	å°†æ•´ä¸ªè¯·æ±‚æ˜¾ç¤ºä¸ºçº¯æ–‡æœ¬
	
ç§»åŠ¨ç«¯appæ•°æ®æŠ“å–
æ–¹æ³•1 - æ‰‹æœº + Fiddler
	è®¾ç½®æ–¹æ³•è§æ–‡ä»¶å¤¹ - ç§»åŠ¨ç«¯æŠ“åŒ…é…ç½®
	
æ–¹æ³•2 - F12æµè§ˆå™¨å·¥å…·
	æœ‰é“ç¿»è¯‘æ‰‹æœºç‰ˆç ´è§£æ¡ˆä¾‹
	

------------------------------------------------------------------

çˆ¬è™«æ€»ç»“

å¸¸è§åçˆ¬ç­–ç•¥
1ã€Headers : æœ€åŸºæœ¬çš„åçˆ¬æ‰‹æ®µï¼Œä¸€èˆ¬è¢«å…³æ³¨çš„å˜é‡æ˜¯UserAgentå’ŒRefererï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æµè§ˆå™¨;
2ã€UA : å»ºç«‹User-Agentæ± ï¼Œæ¯æ¬¡è®¿é—®é¡µé¢éšæœºåˆ‡æ¢;
3ã€æ‹‰é»‘é«˜é¢‘è®¿é—®IP:æ•°æ®é‡å¤§ç”¨ä»£ç†IPæ± ä¼ªè£…æˆå¤šä¸ªè®¿é—®è€…ï¼Œä¹Ÿå¯æ§åˆ¶çˆ¬å–é€Ÿåº¦;
4ã€Cookies : å»ºç«‹æœ‰æ•ˆçš„cookieæ± ï¼Œæ¯æ¬¡è®¿é—®éšæœºåˆ‡æ¢ ;
5ã€éªŒè¯ç  : 
	éªŒè¯ç æ•°é‡è¾ƒå°‘å¯äººå·¥å¡«å†™
	å›¾å½¢éªŒè¯ç å¯ä½¿ç”¨tesseractè¯†åˆ«
	å…¶ä»–æƒ…å†µåªèƒ½åœ¨çº¿æ‰“ç ã€äººå·¥æ‰“ç å’Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
6ã€åŠ¨æ€ç”Ÿæˆ
	ä¸€èˆ¬ç”±jsåŠ¨æ€ç”Ÿæˆçš„æ•°æ®éƒ½æ˜¯å‘ç‰¹å®šçš„åœ°å€å‘getè¯·æ±‚å¾—åˆ°çš„ï¼Œè¿”å›çš„ä¸€èˆ¬æ˜¯json
7ã€ç­¾ååŠjsåŠ å¯†
	ä¸€èˆ¬ä¸ºæœ¬åœ°jsåŠ å¯† ï¼ŒæŸ¥æ‰¾æœ¬åœ°jsæ–‡ä»¶ã€åˆ†æ ï¼Œæˆ–è€…ä½¿ç”¨execjsæ¨¡å—æ‰§è¡Œjsä»£ç 
8ã€jsè°ƒæ•´é¡µé¢ç»“æ„
9ã€jsåœ¨å“åº”ä¸­æŒ‡å‘æ–°çš„åœ°å€

åˆ†å¸ƒå¼çˆ¬è™«çš„åŸç† -> å¤šå°ä¸»æœºå…±äº«ä¸€ä¸ªçˆ¬å–é˜Ÿåˆ—

-----------------------------------------------------------------------------------------------------------------------

Pythonå­¦ä¹ ä¹‹cookiesåŠsessionç”¨æ³•
	å½“æƒ³åˆ©ç”¨Pythonåœ¨ç½‘é¡µä¸Šå‘è¡¨è¯„è®ºæ—¶ï¼Œéœ€è¦å¸å·å¯†ç ç™»å½•ä¿¡æ¯ï¼Œè¿™ä¸ªæ—¶å€™ç”¨requests.getè¯·æ±‚çš„è¯ï¼Œå¸å·å’Œå¯†ç  ä¼šå…¨éƒ¨æ˜¾ç¤ºåœ¨ç½‘å€ä¸Šï¼Œéå¸¸ä¸ç§‘å­¦ï¼
	è¿™ä¸ªæ—¶å€™éœ€è¦ç”¨postè¯·æ±‚ï¼Œå¯ä»¥è¿™æ ·ç†è§£ï¼Œgetæ˜¯æ˜æ–‡æ˜¾ç¤ºï¼Œpostæ˜¯éæ˜æ–‡æ˜¾ç¤ºã€‚
	

	é€šå¸¸ï¼Œgetè¯·æ±‚ä¼šåº”ç”¨äºè·å–ç½‘é¡µæ•°æ®ï¼Œæ¯”å¦‚æˆ‘ä»¬ä¹‹å‰å­¦ä¹ çš„requests.get()ã€‚
	postè¯·æ±‚åˆ™åº”ç”¨äºå‘ç½‘é¡µæäº¤æ•°æ®ï¼Œæ¯”å¦‚æäº¤è¡¨å•ç±»å‹æ•°æ®(åƒå¸å·å¯†ç å°±æ˜¯ç½‘é¡µè¡¨å•æ•°æ®)ã€‚
	åœ¨postè¯·æ±‚é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨dataæ¥ä¼ é€’å‚æ•°ï¼Œå…¶ç”¨æ³•å’Œparamséå¸¸ç›¸åƒã€‚
	
	å½“ç”¨åˆ°postè¯·æ±‚æ—¶ï¼Œéœ€è¦äº†è§£ä¸¤ä¸ªå‚æ•°ï¼Œcookieså’Œsessionã€‚
	
	1ã€cookiesåŠå…¶ç”¨æ³•
		å½“ç™»å½•ä¸€ä¸ªç½‘ç«™ï¼Œç™»å½•é¡µé¢ä¼šæœ‰ä¸€ä¸ªå¯å‹¾é€‰çš„é€‰é¡¹"è®°ä½æˆ‘"ï¼Œå¦‚æœä½ å‹¾é€‰äº†ï¼Œä»¥åä½ å†æ‰“å¼€è¿™ä¸ªç½‘ç«™å°±ä¼šè‡ªåŠ¨ç™»å½•ï¼Œè¿™å°±æ˜¯cookieåœ¨èµ·ä½œç”¨ã€‚
		æˆ‘ä»¬æƒ³è¦å‘è¡¨è¯„è®ºï¼Œé¦–å…ˆå¾—ç™»å½•ï¼Œå…¶æ¬¡å¾—æå–å’Œè°ƒç”¨ç™»å½•çš„cookiesï¼Œç„¶åè¿˜éœ€è¦è¯„è®ºçš„å‚æ•°ï¼Œæ‰èƒ½å‘èµ·è¯„è®ºçš„è¯·æ±‚ã€‚
			â€”â€”â€”â€” æå–cookiesæ–¹æ³•: è°ƒç”¨requestså¯¹è±¡çš„cookieså±æ€§è·å¾—ç™»å½•çš„cookiesï¼Œå¹¶èµ‹å€¼ç»™å˜é‡cookiesï¼Œæœ€åå¸¦ç€ cookieså»è¯·æ±‚å‘è¡¨è¯„è®ºã€‚
		   eg:	login_in = requests.post(url,headers=headers,data=data)
				# ç”¨requests.postå‘èµ·è¯·æ±‚ï¼Œæ”¾å…¥å‚æ•°:è¯·æ±‚ç™»å½•çš„ç½‘å€ã€è¯·æ±‚å¤´å’Œç™»å½•å‚æ•°ï¼Œç„¶åèµ‹å€¼ç»™login_in;
				cookies = login_in.cookies
				# æå–cookiesçš„æ–¹æ³•: è°ƒç”¨requestså¯¹è±¡(login_in)çš„cookieså±æ€§è·å¾—ç™»å½•çš„cookiesï¼Œå¹¶å¤åˆ¶ç»™å˜é‡cookies
				
	2ã€sessionåŠå…¶ç”¨æ³•
		sessionæ˜¯ä¼šè¯è¿‡ç¨‹ä¸­ï¼ŒæœåŠ¡å™¨ç”¨æ¥è®°å½•ç‰¹å®šç”¨æˆ·ä¼šè¯çš„ä¿¡æ¯ã€‚sessionå’Œcookieså…³ç³»å¯†åˆ‡â€”â€”â€”â€”cookiesä¸­å­˜å‚¨ç€sessionçš„ç¼–ç ä¿¡æ¯ï¼Œsessionä¸­åˆå­˜å‚¨äº†cookiesçš„ä¿¡æ¯ã€‚

pythonä¸­cookieå’Œsessionçš„åŒºåˆ«:
åŒºåˆ«:
1ã€cookieæ•°æ®å­˜å‚¨åœ¨å®¢æˆ·æµè§ˆå™¨ä¸Šï¼Œsessionåœ¨æœåŠ¡å™¨ä¸Š;
2ã€cookieä¸å®‰å…¨ï¼Œsessionè¾ƒå®‰å…¨; -> ä»–äººå¯ä»¥åˆ†æä¿ç®¡åœ¨å½“åœ°çš„cookieï¼Œæ¬ºéª—cookieï¼Œè€ƒè™‘åˆ°å®‰å…¨åº”è¯¥ä½¿ç”¨session;
3ã€è®¿é—®å¢åŠ é€‰cookie; -> sessionåœ¨ ä¸€å®šæ—¶é—´å†…ä¿å­˜åœ¨æœåŠ¡å™¨ä¸Šã€‚è®¿é—®å¢åŠ æ—¶ï¼Œè€ƒè™‘åˆ°æœåŠ¡å™¨çš„æ€§èƒ½å‡è½»ï¼Œå¿…é¡»ä½¿ç”¨cookie;
4ã€cookieä¿å­˜ä¸è¶…è¿‡4k; -> å•ä¸ªcookieä¿å­˜çš„æ•°æ®ä¸å¾—è¶…è¿‡4kã€‚è®¸å¤šæµè§ˆå™¨é™åˆ¶äº†ä¸€ä¸ªç½‘ç«™ æœ€å¤šä¿å­˜20ä¸ªcookie;
å»ºè®®: å°†ç™»å½•ä¿¡æ¯ç­‰é‡è¦ä¿¡æ¯å­˜å‚¨åœ¨SEESIONçš„å…¶ä»–ä¿¡æ¯ä¸­ï¼Œå¯ä»¥å­˜å‚¨åœ¨cookieä¸­ã€‚

sessionçš„ä¸€ç§ç”¨æ³•æ€è·¯:
1ã€å®ä¾‹åŒ– sessionå¯¹è±¡
self.session = requests.session()

2ã€å…ˆpostï¼ŒæŠŠcookieä¿å­˜åœ¨sessionå¯¹è±¡ä¸­ - ä¼šè¯ä¿æŒ
self.session.post(url=self.post_url,data=data)

3ã€å†getï¼Œæ­£å¸¸æŠ“å–æ•°æ®
html = self.session.get(url=self.get_url).text
		
-----------------------------------------------------------------------------------------------------------------------

requests.session()å‘é€è¯·æ±‚å’Œä½¿ç”¨requestsç›´æ¥å‘é€è¯·æ±‚çš„åŒºåˆ«:
ä¸€ã€Session
åœ¨requestsé‡Œï¼Œsessionå¯¹è±¡æ˜¯ä¸€ä¸ªéå¸¸å¸¸ç”¨çš„å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡ä»£è¡¨ä¸€æ¬¡ç”¨æˆ·å¯¹ä¼šè¯:ä»å®¢æˆ·ç«¯æµè§ˆå™¨è¿æ¥æœåŠ¡å™¨å¼€å§‹ï¼Œåˆ°å®¢æˆ·ç«¯æµè§ˆå™¨ä¸æœåŠ¡å™¨æ–­å¼€;
ä¼šè¯èƒ½å¤Ÿè®©æˆ‘ä»¬åœ¨è·¨è¯·æ±‚çš„æ—¶å€™ä¿æŒæŸäº›å‚æ•°ï¼Œæ¯”å¦‚åœ¨åŒä¸€ä¸ªsessionå®ä¾‹å‘å‡ºçš„æ‰€æœ‰è¯·æ±‚ä¹‹é—´ä¿æŒcookieä¿¡æ¯ã€‚
1ã€åˆ›å»ºsessionå¯¹è±¡
session = requests.session() -> å¾—åˆ°sessionå¯¹è±¡åï¼Œå°±å¯ä»¥è°ƒç”¨è¯¥å¯¹è±¡ä¸­çš„æ–¹æ³•æ¥å‘é€è¯·æ±‚äº†;
response1 = session.get(url,params,headers)
response2 = session.post(url,data,json,headers)
é€šè¿‡sessionæ¥å‘é€getã€postã€deleteã€putç­‰è¯·æ±‚å¹¶è·å–å“åº”;

äºŒã€requests
requestsæ˜¯Pythonçš„ä¸€ä¸ªç¬¬ä¸‰æ–¹åº“ï¼Œä¸»è¦ç”¨äºå‘é€ç½‘ç»œè¯·æ±‚ï¼Œæ¯”å¦‚getã€postç­‰è¯·æ±‚å·²è¾¾åˆ°è·å–ç½‘ç»œå“åº”çš„ç›®çš„

import requests
response1 = requests.get(url,params,headers,cookies)
response2 = requests.post(url,data,json,headers,cookies)
### putã€deleteç­‰è¯·æ±‚æ–¹æ³•ç±»ä¼¼

ä¸‰ã€sessionå¯¹è±¡å’Œrequestsä¸¤ç§æ–¹æ³•å‘é€çš„è¯·æ±‚çš„åŒºåˆ«:
1ã€åœºæ™¯
	->ç™»å½•æŸå•†åŸ
	->æŸ¥è¯¢æˆ‘çš„è®¢å•æ•°æ® 
	
2ã€ä¸šåŠ¡ä»£ç åˆ†æ
	->é¦–å…ˆè¿™é‡Œæ¶‰åŠåˆ°ä¸¤ä¸ªæ¥å£ï¼Œä¸€ä¸ª"ç™»å½•æ¥å£",å¦å¤–ä¸€ä¸ªæ˜¯"æŸ¥è¯¢è®¢å•"æ¥å£ã€‚
	->å¸¸è§„æ“ä½œæ˜¯æˆ‘ä»¬è°ƒç”¨ç™»å½•æ¥å£æ¥è·å–å“åº”çš„cookieä¿¡æ¯ ã€‚
	->ç„¶åæ‹¿è¿™ä¸ªcookieä¿¡æ¯ä½œä¸ºä¸‹ä¸€æ¬¡è¯·æ±‚çš„å‚æ•°(cookieå¸¦æœ‰å½“å‰ç™»å½•äººä¿¡æ¯)æ¥è¯·æ±‚æŸ¥è¯¢è®¢å•æ¥å£ã€‚
	
å¸¸è§„ä»£ç å¦‚ä¸‹:
import requests
#ç™»å½•æ¥å£
response1 = requests.get(url_login,params,headers)
#è·å–cookieä¿¡æ¯
cookies = response1.cookies
#å¾—åˆ°cookiesæ˜¯ä¸€ä¸ªå­—å…¸ç±»å‹
cookie = cookies.get('cookiesçš„key')
#è¯·æ±‚æŸ¥è¯¢æ¥å£ 
response2 = requests.get(search_url,params,headers,cookies = cookie)
#æŸ¥çœ‹å“åº”çš„ç»“æœ
response2.json()

ä½¿ç”¨sessionä»£ç å¦‚ä¸‹:
import requests

#è·å–sessionå¯¹è±¡
session = requests.session()
#ç™»å½• æ¥å£
response1 = session.get(url_login,params,headers)
#è¯·æ±‚ æŸ¥è¯¢æ¥å£
response2 = session.get(search_url,params,headers)
#æŸ¥çœ‹å“åº”çš„ç»“æœ 
response2.json()

åŒºåˆ«:
	1ã€é€šè¿‡ä»£ç å¯¹æ¯”å¯ä»¥å‘ç°ä½¿ç”¨sessionå¯¹è±¡æ•ˆæœä¼šæ›´å¥½ï¼Œä¸ç”¨æ¯æ¬¡éƒ½æŠŠcookieä¿¡æ¯æ”¾åˆ°è¯·æ±‚å†…å®¹ä¸­äº†;
	2ã€sessionå¯¹è±¡èƒ½å¤Ÿè‡ªåŠ¨è·å–åˆ°cookieï¼Œå¹¶ä¸”å¯ä»¥åœ¨ä¸‹æ¬¡çš„è¯·æ±‚ä¸­è‡ªåŠ¨å¸¦ä¸Šæ‰€å¾—çš„cookieä¿¡æ¯ï¼Œä¸ç”¨äººä¸ºåœ°å»å¡«å†™;

-----------------------------------------------------------------------------------------------------------------------

äº‘æ‰“ç ï¼Œæˆªå–éªŒè¯ç å›¾ç‰‡ï¼Œå¹¶å®ç°éªŒè¯ç ä¿¡æ¯è·å–ï¼Œæ­¥éª¤:
1ã€å®šä½éªŒè¯ç å›¾ç‰‡èŠ‚ç‚¹ - x yåæ ‡(å·¦ä¸Šè§’)
	location = self.browser.find_element(By.XPATH,xpath_bds).location
	
2ã€è·å–å®½åº¦å’Œé«˜åº¦ 
	size = self.browser.find_element(By.XPATH,xpath_bds).size
	
3ã€å·¦ä¸Šè§’x y åæ ‡
    left = location['x']
    top = location['y']
    
4ã€å³ä¸‹è§’x y åæ ‡
    right = left + size['width']
    bottom = top + size['height']
    
5ã€æˆªå–éªŒè¯ç å›¾ç‰‡(crop((å…ƒç»„:å››ä¸ªå‚æ•°))) : å¯¹å›¾ç‰‡è¿›è¡Œå‰ªåˆ‡
	self.browser.get(self.url)
    self.browser.save_screenshot('index.png')

    img = Image.open('index.png').crop((left,top,right,bottom))
    img.save('yzm.png')

6ã€åœ¨çº¿æ‰“ç 
    balance = self.yundama.get_balance()
    print('æ‰“ç ä½™é¢:',balance)

    result = self.yundama.get_code_result('yzm.png')
    print(result)

-----------------------------------------------------------------------------------------------------------------------

å­—å…¸æ•°æ®ç±»å‹è‡ªå¸¦çš„update()æ–¹æ³•ï¼Œå¯ä»¥ç”¨æ¥å¿«é€Ÿæ›´æ–°å­—å…¸çš„æ•°æ®ï¼(é‡è¦)

-----------------------------------------------------------------------------------------------------------------------

Linuxå‘½ä»¤:
netstat -tulpn  -> æ˜¾ç¤ºå½“å‰æ“ä½œç³»ç»Ÿä¸­è¿è¡Œçš„è¿›ç¨‹çš„PID/Program name

-----------------------------------------------------------------------------------------------------------------------

tips:

1ã€æœ‰ä¸€ä¸ªå®˜æ–¹ç½‘ç«™ï¼Œå¯ä»¥å°†cURL(bash),ç›´æ¥è½¬æ¢æˆrequests.get()çš„ç›¸å…³ä»£ç  -> çŒ¿äººå­¦

2ã€åœ¨çˆ¬å–éœ€è¦è¿›è¡ŒloginIDå’Œpasswordç™»å½•çš„ç½‘é¡µæ—¶ï¼Œå¯ä»¥é€šè¿‡å°è¯•è¾“å…¥loginIDå’Œpasswordï¼Œé€šè¿‡F12å¼€å‘è€…å·¥å…·ï¼Œæ¥æŸ¥çœ‹ç™»å½•è¯·æ±‚urlçš„è¯·æ±‚æ–¹å¼ã€å‚æ•°åŠ å¯†æ–¹å¼ã€è¯·æ±‚å¤´æ•°æ®ç­‰ç­‰;

3ã€å¦‚ä½•å¯»æ‰¾ç™»å½•ç½‘é¡µæ—¶çš„å‚æ•°åŠ å¯†ä¿¡æ¯ï¼Ÿ-> é€šè¿‡åœ¨JSæ–‡ä»¶åŠ å…¥æ–­ç‚¹ï¼Œåœ¨ç‚¹å‡»ç™»å½•æŒ‰é’®æ—¶ï¼Œçœ‹æ–­ç‚¹æ˜¯å¦è¢«ä½¿ç”¨ï¼Œå¦‚æœç¨‹åºåˆ°äº†æ–­ç‚¹å¤„åœä½äº†ï¼Œè¯æ˜è¯¥æ®µJSä»£ç åœ¨ç™»å½•ç½‘é¡µæ—¶è¢«æ‰§è¡Œäº†ã€‚

4ã€å¦‚æœå‘ç°ï¼Œresponseè¿”å›200ï¼Œä½†æ˜¯response.textæˆ–è€…response.contentè¿”å›ç©ºã€‚è¿™ä¸ªæ—¶å€™ï¼Œéœ€è¦æ·»åŠ å®Œå–„headerså¤´éƒ¨ä¿¡æ¯ã€‚

5ã€åœ¨ä½¿ç”¨execjs()æ–¹æ³•è¿›è¡Œjsä»£ç é€†å‘çš„æ—¶å€™ï¼Œå¦‚æœå‘ç°æœ‰äº›å‚æ•°éœ€è¦è°ƒç”¨ï¼Œå¯ä»¥åœ¨å“åº”é¡µé¢ä¸­è¿›è¡ŒåŒ¹é…ï¼ŒæŸ¥çœ‹æœåŠ¡å™¨æ˜¯å¦å°†å‚æ•°ç»™åˆ°å‰ç«¯é¡µé¢äº†ã€‚  ç‰¹åˆ«åœ°:å¦‚æœå‘ç°è¯¸å¦‚window.xxx.xxxï¼Œå¤§å¤šéƒ½æ˜¯å¯ä»¥åœ¨å“åº”é¡µé¢ä¸­æŸ¥åˆ°çš„ã€‚

6ã€æ¯æ¬¡F12ä¸­æŠ“å®ŒåŒ…é‡Œé¢ï¼Œçœ‹åˆ°æœ‰åŠ¨æ€çš„å‚æ•°ï¼Œç¬¬ä¸€ååº”è¦å…ˆçœ‹æºä»£ç ä¸­æ˜¯å¦æœ‰åŠ è½½ï¼Œç¬¬äºŒæ­¥æ‰æ˜¯å»jsä¸­æ‰¾å¯¹åº”ç¨‹åºã€‚

7ã€å¦‚æœç½‘ç«™å±è”½äº†å³é”®æŸ¥çœ‹æºä»£ç ï¼Œå¯ä»¥åœ¨æŠ“åŒ…å·¥å…·é‡Œç‚¹å‡»ç½‘ç»œåŒ…åï¼Œè¿›è¡ŒresponseæŸ¥çœ‹ã€‚



#### å…³äºå­—ä½“åçˆ¬:

1ã€å¾…çˆ¬å–æ•°æ®çš„é¡µé¢ä¸­ï¼Œä¸€èˆ¬ä¼šä¼´æœ‰å­—ä½“åº“çš„è¿”å› -> å¯èƒ½æ˜¯è·Ÿç€ç½‘é¡µè¿”å›çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯è¿”å›ä¸€ä¸ªé“¾æ¥URLï¼›

â€‹	  å¯ä»¥åœ¨ç½‘é¡µä¸­æœç´¢: ttf,woff,woff2

2ã€éœ€è¦å¯¼å…¥å­—ä½“åº“from fontTools.ttLib import TTFontï¼›

3ã€æ³¨æ„æœ€å¥½ä¿å­˜ä¸€ä»½htmlç½‘é¡µä¿¡æ¯æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œé˜²æ­¢ç½‘ç»œæœåŠ¡å™¨å°é”IPï¼›
4ã€å­—ä½“åº“æ–‡ä»¶.ttfæ–‡ä»¶çš„å†…å®¹ï¼Œä¸€èˆ¬ä¿å­˜åœ¨htmlç½‘é¡µä¿¡æ¯ä¸­ï¼Œä¸”æ˜¯ç¼–ç è¿‡åçš„ä¿¡æ¯ï¼Œéœ€è¦åœ¨Pythonè„šæœ¬ä¸­é‡æ–°è§£ç ï¼Œå¹¶ä¿å­˜ä¸º.ttfæ–‡ä»¶åˆ°æœ¬åœ°ï¼›

5ã€base64çš„è§£ç æŒ‡ä»¤ä¸º :base64.b64decode(ttf) -> å¾—åˆ°äºŒè¿›åˆ¶ttfå­—ä½“åº“è§£ç ä¿¡æ¯ï¼Œä¿å­˜åˆ°æœ¬åœ°.ttfæ–‡ä»¶ä¸­ï¼›

6ã€è°ƒç”¨TTFont('.ttfæ–‡ä»¶ä¿å­˜è·¯å¾„')ï¼Œå®ä¾‹åŒ–å­—ä½“åº“å¯¹è±¡ï¼›

7ã€BestCmap = font.getBestCmap() -> unicodeä¸å›¾å½¢ç¼–å·çš„å¯¹åº”å…³ç³»

â€‹	  GlyphMap = font.getReverseGlyphMap() -> å…·ä½“çš„å€¼å’Œå›¾å½¢ç¼–å·çš„å…³ç³»

8ã€æœ€åå¼€å§‹æ ¹æ®mapä¸­çš„å¯¹åº”å…³ç³»ï¼Œå¾—åˆ°åçˆ¬ä»·æ ¼å¯¹åº”çš„çœŸå®ä»·æ ¼æ•°æ®ï¼›

----------------------------------------------------------------------------------------------------------------------------------------------------------

å¦‚æœæ˜¯.woff2çš„å­—ä½“åº“æ–‡ä»¶ ï¼Œåœ¨ä½¿ç”¨å­—ä½“åº“æ–‡ä»¶æ—¶ï¼Œéœ€è¦è¿›è¡ŒæŒ‡çº¹æ„å»ºã€‚

```python
from fontTools.ttLib import TTFont
font = TTFont('gz.woff2')
print(font.getGlyphNames())
names = font.getGlyphNames()[3:-2]
names.remove('uniE4D9')
for name in names:
    # è·å–glyfèŠ‚TTGlyphå­—ä½“xã€yåæ ‡ä¿¡æ¯
    print(name)
    aa = font['glyf'][name].coordinates
    print(aa)
    bb = font['glyf'][name].flags  # flagsæ˜¯æŒ‡çº¹
    print(bb)
    print()
```

-----

### ä½¿ç”¨mitmproxyçš„æ³¨æ„äº‹é¡¹ï¼š

1ã€è¦åŠ è½½mitmproxyçš„è¯ä¹¦ï¼š

linuxç³»ç»Ÿç¯å¢ƒä¸‹,pip install mitmproxyåï¼Œåœ¨ç»ˆç«¯è¿è¡Œmitmdumpåï¼Œåœ¨æµè§ˆå™¨ç½‘é¡µè¾“å…¥mitm.itï¼Œä¸‹è½½è¯ä¹¦ï¼›

mitm.itä¸Šæœ‰æ•™ç¨‹ï¼ŒæŒ‡å¯¼è¯ä¹¦çš„ä¿å­˜ä½ç½®ã€‚

è¯ä¹¦è¿˜éœ€è¦åŠ è½½åˆ°Chromeæµè§ˆå™¨ä¸­(é‡è¦)ã€‚

2ã€è¦æ‰“å¼€æœ¬åœ°è®¾å¤‡çš„äººå·¥ä»£ç†è®¾ç½®ã€‚httpã€httpséƒ½éœ€è¦è®¾ç½®ã€‚

----

### Scrapyæ¡†æ¶ä½¿ç”¨æµç¨‹:

ä¸€ã€åˆ›å»ºé¡¹ç›®:
	åˆ›å»ºé¡¹ç›®çš„ä¸¤ç§æ–¹å¼: scrapy startproject db

â€‹										scrapy startproject inner_folder outer_folder

äºŒã€è¿›å…¥åˆ°é¡¹ç›®ä¸­ï¼Œåˆ›å»ºçˆ¬è™«æ–‡ä»¶:scrapy genspider db250 movie.douban.com

ä¸‰ã€ä¿®æ”¹start_urlsè®¿é—®çš„urlä¸º:['https://movie.douban.com/top250']

å››ã€settings.pyæ–‡ä»¶è®¾ç½®

â€‹	1ã€ä¸éµå®ˆrobotsåè®®ï¼š

â€‹			ROBOTSTXT_OBEY = False

â€‹	2ã€è®¾ç½®è¯·æ±‚å¤´ï¼š

â€‹			DEFAULT_REQUEST_HEADERS = {

â€‹				è¿™é‡Œå…·ä½“å†…å®¹çœç•¥ï¼ŒæŒ‰ç…§ç½‘ç«™ä¸Šçš„è¯·æ±‚å¤´ä¿¡æ¯è¿›è¡Œæ›´æ–°å³å¯

}

äº”ã€ä¿®æ”¹parseæ–¹æ³•ï¼š

â€‹	1ã€parseæ–¹æ³•ä¸­çš„responseæ˜¯è¯·æ±‚ä¸Šé¢urlè¿”å›çš„htmlå†…å®¹ï¼›

â€‹	2ã€ä½¿ç”¨xpathè§£æhtmlä¸­çš„æ•°æ®å†…å®¹ï¼Œè§£ææ–¹æ³•å’Œlxmlä¸­çš„è§£æç±»ä¼¼ï¼Œå¤šä¸€ä¸ªget()æ–¹æ³•ï¼›

â€‹	3ã€å¯¼å…¥items.pyæ–‡ä»¶ä¸­çš„DbItemç±»ï¼Œè¿›è¡Œå®ä¾‹åŒ–è¿™ä¸ªç±»å¯¹è±¡ï¼Œç”¨æ¥å­˜å‚¨æäº¤ç»™ç®¡é“pipelines.pyæ•°æ®ï¼Œç±»ä¼¼äº

â€‹			from ..items import DbItem

â€‹			def parse(self,response):

â€‹					dbitem = DbItem()

â€‹					dbitem['movie_name'] = movie_name

â€‹					dbitem['d_name'] = d_name

â€‹					dbitem['price'] = price

â€‹					yield dbitem

â€‹				

â€‹	4ã€tiemsæ„é€ ï¼šå­—æ®µåè¦å’Œå‰é¢ä¼ é€’æ—¶çš„ä¸€æ ·

â€‹		class DbItem(scrapy.Item):

â€‹				movie_name = scrapy.Field()

â€‹				d_name = scrapy.Field()

â€‹				price = scrapy.Field()



å…­ã€pipelineä¿®æ”¹

â€‹	1ã€æ¿€æ´»ç®¡é“ï¼š

â€‹			åœ¨settingsä¸­æ‰¾åˆ°å¦‚ä¸‹å†…å®¹ï¼Œå–æ¶ˆæ³¨é‡Šï¼š

â€‹					ITEM_PIPELINES = {

â€‹							'db.pipelines.DbPipeline':300,

}

â€‹	2ã€ä¿®æ”¹DbPipelineç±»ä¸­çš„ä¸‰ä¸ªæ–¹æ³• ï¼Œæ–¹æ³•åéƒ½æ˜¯å›ºå®šçš„ï¼Œæ–‡ä»¶ä¿å­˜çš„ä½¿ç”¨æ–¹å¼çœ‹pipelines.pyæ–‡ä»¶

â€‹			# çˆ¬è™«å¼€å§‹çš„æ—¶å€™æ‰§è¡Œ   ä¸€èˆ¬ç”¨æ¥æ‰“å¼€æ–‡ä»¶ï¼Œé“¾æ¥æ•°æ®åº“  

â€‹			def open_spider(self,spider):

â€‹					pass

â€‹			 # æ¥æ”¶çˆ¬è™«æ–‡ä»¶ä¼ é€’è¿‡æ¥çš„æ•°æ®(itemæ˜¯æ•°æ®)ï¼Œä»–ä¸æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œéœ€è¦å¼ºè½¬ä¸ºå­—å…¸(dict(item))ï¼Œç„¶å

â€‹			def process_item(self,item,spider):

â€‹					return item

â€‹             # çˆ¬è™«ç»“æŸçš„æ—¶å€™æ‰§è¡Œï¼Œä¸€èˆ¬ç”¨æ¥å…³é—­æ–‡ä»¶ï¼Œå…³é—­æ•°æ®åº“

â€‹			def close_spider(self,spider):

â€‹					pass



ä¸ƒã€å¯åŠ¨çˆ¬è™«

å‘½ä»¤è¡Œè¾“å…¥ï¼š scrapy crawl douban1

å…«ã€ç¿»é¡µ

å¤šé¡µæŠ“å–

è¿›è¡Œæ¯”è¾ƒå¤šä¸ªé¡µé¢çš„urlä¸åŒä¹‹å¤„ï¼šåˆ†åˆ«æ˜¯ 0 -> 25 -> 50ï¼Œè¯´æ˜æ¯é¡µæ˜¯25çš„æ•°éš”

https://movie.douban.com/top250?start=0&filter=

https://movie.douban.com/top250?start=25&filter=

https://movie.douban.com/top250?start=50&filter=



å¤šé¡µæ„é€ æ³•ä¸€ï¼šç›´æ¥æ„é€ å…¶ä»–é¡µé¢çš„url

start_urls = [f'https://movie.douban.com/top250?start={page*25}&filter=' for page in range(3)]  # è®¾ç½®3é¡µ

è¿™é‡Œéœ€è¦æ³¨æ„ï¼šç”¨æ–¹æ³•ä¸€è¿›è¡Œurlè¯·æ±‚ï¼Œç”±äºScrapyæ˜¯ä¸€ä¸ªå¼‚æ­¥çš„æ¡†æ¶ï¼Œæ‰€æœ‰start_urlsä¸­çš„urlè°å…ˆåŠ è½½å®Œï¼Œåˆ™è°å…ˆä¿å­˜ï¼(é‡è¦ï¼)



å¤šé¡µæ„é€ æ³•äºŒï¼šé‡å†™start_requestsæ–¹æ³•

def start_requests(self):

â€‹		for page in range(3):   # è®¾ç½®3é¡µ
â€‹				page_url = f'https://movie.douban.com/top250?start={page*25}&filter='

â€‹				yield scrapy.Request(page_url,callback=self.parse)



å¤šé¡µæ„é€ æ³•ä¸‰ï¼š

self.page_num += 1

page_url = f'https://movie.douban.com/top250?start={self.page_num*25}&filter='

yield scrapy.Request(page_url)



# Scrapyæ¡†æ¶ä½¿ç”¨æ³¨æ„:

1ã€ä½¿ç”¨scrapyæ¡†æ¶ï¼Œå¦‚æœè¦ä½¿ç”¨è‡ªå®šä¹‰cookiesï¼Œéœ€è¦å°†settings.pyä¸­çš„enable_cookiesç½®ä¸ºTrue;

2ã€ä¸­é—´ä»¶çš„è®¾ç½®ä¸å¼€å¯åœ¨Settingsæ¨¡å—ä¸­ï¼Œä»¥å­—å…¸çš„å½¢å¼è¡¨ç°ï¼Œkeyä¸ºä¸­é—´ä»¶æ¨¡å—çš„å¯¼å…¥è·¯å¾„(åŒimportè·¯å¾„)ï¼Œvalueä¸ºä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºè¯¥ä¸­é—´ä»¶çš„æƒé‡ã€‚å¯¹äºä¸‹è½½ä¸­é—´ä»¶æ¥è¯´ï¼Œæƒé‡è¶Šå°ï¼Œç¦»å¼•æ“è¶Šè¿‘ï¼›æƒé‡è¶Šå¤§ï¼Œè¶Šé è¿‘ä¸‹è½½å™¨ã€‚åœ¨å¤„ç†Requestå’ŒResponseçš„æ—¶å€™éƒ½æ˜¯ç»è¿‡é¡ºåºä¾æ¬¡è¿›è¡Œã€‚

â€‹		ä¸‹è½½ä¸­é—´ä»¶æ˜¯Scrapyæ¡†æ¶ä¸­éå¸¸çµæ´»çš„éƒ¨åˆ†ï¼Œå¯ä»¥é«˜åº¦è‡ªç”±åœ°å®šåˆ¶æ¡†æ¶å¤„ç†è¯·æ±‚å’Œå“åº”çš„è¿‡ç¨‹ï¼Œä½†æ˜¯ç”¨èµ·æ¥ç¨æœ‰äº›å¤æ‚ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸­é—´ä»¶ä¸­çš„æ–¹æ³•: process_requestã€process_responseå’Œprocess_exceptionä¸‰ä¸ªæ–¹æ³•ã€‚

- process_requeståœ¨è¯·æ±‚ç»è¿‡æ—¶è¢«è°ƒç”¨ï¼Œå®ƒæ¥å—ä¸¤ä¸ªå‚æ•°ï¼šrequestå’Œspiderï¼Œåœ¨å¼•æ“è°ƒç”¨çš„æ—¶å€™ä¼šé»˜è®¤ä¼ å…¥ï¼Œåˆ†åˆ«ä¸ºå¾…å¤„ç†çš„requestå¯¹è±¡å’Œäº§ç”Ÿè¯¥è¯·æ±‚çš„spiderå¯¹è±¡ï¼Œåœ¨è¿™ä¸ªæ–¹æ³•ä¸­å¯ä»¥å¯¹requestå¯¹è±¡è¿›è¡ŒåŠ å·¥ï¼Œå¢åŠ å…¶å±æ€§æˆ–åœ¨METAå­—å…¸ä¸­å¢åŠ å­—æ®µæ¥ä¼ é€’ä¿¡æ¯ã€‚è¿™ä¸ªæ–¹æ³•å…è®¸è¿”å›requestå¯¹è±¡æˆ–è¿”å›ä¸€ä¸ªIgnoreRequestå¼‚å¸¸æˆ–è€…æ— è¿”å›(å³è¿”å›None)ã€‚åœ¨è¿”å›Requestå¯¹è±¡æ—¶ï¼Œè¿™ä¸ªå¯¹è±¡ä¸ä¼šå†ä¼ é€’ç»™æ¥ä¸‹æ¥çš„ä¸­é—´ä»¶ï¼Œè€Œæ˜¯é‡æ–°è¿›å…¥è°ƒåº¦å™¨ é˜Ÿåˆ—ä¸­ç­‰å¾…å¼•æ“è°ƒåº¦ã€‚è¿”å›å¼‚å¸¸æ—¶ï¼Œè¯¥å¼‚å¸¸ä¼šä½œä¸ºä¸­é—´ä»¶process_exceptionæ–¹æ³•çš„å‚æ•°ä¼ å…¥ï¼Œè‹¥æ— process_exceptionæ–¹æ³•ï¼Œåˆ™ä¼šäº¤ç»™è¯¥è¯·æ±‚err_backç»‘å®šçš„æ–¹æ³•å¤„ç†ã€‚è‹¥æ— è¿”å›åˆ™å°†requestå¯¹è±¡äº¤ç»™ä¸‹ä¸€ä¸ªä¸­é—´ä»¶å¤„ç†ã€‚
- process_responseåœ¨å“åº”è¢«è¿”å›ç»è¿‡æ—¶è°ƒç”¨ï¼Œæ¥å—3ä¸ªå‚æ•°ï¼Œåˆ†åˆ«ä¸ºrequestå¯¹è±¡ï¼Œresponseå¯¹è±¡å’Œspiderå¯¹è±¡ã€‚spiderå¯¹è±¡åŒä¸Šï¼Œresponseå¯¹è±¡ä¸ºä¸‹è½½å™¨è¿”å›çš„å“åº”ï¼Œè€Œrequestå¯¹è±¡åˆ™è¡¨ç¤ºäº§ç”Ÿè¯¥å“åº”çš„è¯·æ±‚ï¼Œä¸spiderä¸­çš„responseä¸åŒï¼Œæ­¤å¤„çš„responseå¯¹è±¡ä¸­æ²¡æœ‰requestå±æ€§ï¼Œè€Œæ˜¯åŒä½œä¸ºå‚æ•°ä¼ å…¥ã€‚åœ¨è¿™ä¸ªæ–¹æ³•ä¸­å¯ä»¥å¯¹ä¸‹è½½å™¨è¿”å›çš„å“åº”è¿›è¡Œå¤„ç†ï¼ŒåŒ…æ‹¬å“åº”çš„è¿‡æ»¤ã€æ ¡éªŒï¼Œä»¥åŠä¸process_requestä¸­è€¦åˆçš„æ“ä½œï¼Œå¦‚Cookieæ± å’Œä»£ç†æ± çš„æ“ä½œç­‰ã€‚è¯¥æ–¹æ³•å…è®¸3ç§è¿”å›ï¼šrequestå¯¹è±¡ã€responseå¯¹è±¡å’ŒIgnoreRequestå¼‚å¸¸ã€‚å½“è¿”å›requestå¯¹è±¡æ—¶åŒä¸Šä¸€æ–¹æ³•ï¼Œå½“è¿”å›responseå¯¹è±¡æ—¶ï¼Œè¯¥responseä¼šäº¤ç”±ä¸‹ä¸€ä¸ªä¸­é—´ä»¶ç»§ç»­å¤„ç†ï¼Œè€Œè¿”å›å¼‚å¸¸åˆ™ç›´æ¥äº¤ç»™è¯¥è¯·æ±‚err_backç»‘å®šçš„æ–¹æ³•å¤„ç†ã€‚
- process_exceptionæ–¹æ³•åœ¨ä¸‹è½½å™¨æˆ–process_requestè¿”å›å¼‚å¸¸æ—¶è¢«è°ƒç”¨ï¼Œæ¥å—3ä¸ªå‚æ•°:requestå¯¹è±¡ï¼Œexceptionå¯¹è±¡å’Œspiderå¯¹è±¡ï¼Œå…¶ä¸­requestå¯¹è±¡ä¸ºäº§ç”Ÿè¯¥å¼‚å¸¸çš„è¯·æ±‚ã€‚å®ƒæœ‰3ç§è¿”å›åˆ†åˆ«ä¸ºrequestå¯¹è±¡ã€responseå¯¹è±¡å’Œæ— è¿”å›ã€‚å½“æ— è¿”å›æ—¶ï¼Œè¯¥å¼‚å¸¸å°†è¢«åç»­çš„ä¸­é—´ä»¶å¤„ç†ï¼›å½“è¿”å›requestå¯¹è±¡å’Œresponseå¯¹è±¡æ—¶ï¼Œå‡ä¸ä¼šå†è°ƒç”¨åç»­çš„process_exceptionæ–¹æ³•ï¼Œå‰è€…å°†ä¼šäº¤ç»™è°ƒåº¦å™¨åŠ å…¥é˜Ÿåˆ—ï¼Œåè€…å°†ä¼šäº¤ç”±process_responseå¤„ç†ã€‚å¯ä»¥çœ‹å‡ºï¼Œä¸­é—´ä»¶å°±æ˜¯ä¸€ä¸ªé’©å­ï¼Œå¯ä»¥æ‹¦æˆªå¹¶åŠ å·¥ç”šè‡³æ›¿æ¢è¯·æ±‚å’Œå“åº”ï¼Œå¯ä»¥æŒ‰ç…§æˆ‘ä»¬çš„æ„æ„¿æ¥æ§åˆ¶ä¸‹è½½å™¨æ¥å—å’Œè¿”å›çš„å¯¹è±¡ã€‚è€Œç”±äºçˆ¬è™«å·¥ä½œçš„è¿‡ç¨‹ä¸»è¦æ˜¯å¤„ç†å„ç§è¯·æ±‚å’Œå“åº”ï¼Œæ‰€ä»¥ä¸‹è½½ä¸­é—´ä»¶å¯ä»¥æå¤§åœ°æ§åˆ¶ çˆ¬è™«çš„è¿è¡Œã€‚
- Pipelinesæ•°æ®ç®¡é“æ˜¯Scrapyä¸­å¤„ç†æ•°æ®çš„ç»„ä»¶ã€‚å½“Spiderä¸­çš„æ–¹æ³•yieldä¸€ä¸ªå­—å…¸æˆ–scrapy.itemç±»åŠå…¶å­ç±»æ—¶ï¼Œå¼•æ“ä¼šå°†å…¶äº¤ç»™pipelineså¤„ç†ã€‚Piplineså’Œä¸­é—´ä»¶éå¸¸ç±»ä¼¼ï¼Œæ•°æ®æ¡ç›®å°†ä»¥æ­¤é€šè¿‡pipelineè¿›è¡Œå¤„ç†ï¼Œpipelineçš„æƒé‡è¶Šå°ï¼Œåˆ™è¶Šæ—©è¢«è°ƒç”¨ã€‚ä¸€ä¸ªpipielineéœ€è¦å®ç°process_itemæ–¹æ³•ï¼Œæ¥å—itemï¼Œspiderä¸¤ä¸ªå‚æ•°ï¼Œå…¶ä¸­itemå³ä¸ºæ‰€å¤„ç†çš„æ•°æ®ã€‚è¯¥æ–¹æ³•å¯è¿”å›item(å­—å…¸æˆ–scrapy.itemç±»åŠå…¶å­ç±»)æˆ–DropItemå¼‚å¸¸æˆ–Twisted Deferredå¯¹è±¡ã€‚è¿”å›çš„itemä¼šäº¤ç”±ä¸‹ä¸€ä¸ªpipelineç»§ç»­å¤„ç†ï¼Œè€Œè¿”å›çš„æ˜¯DropItemå¼‚å¸¸æ—¶ï¼Œåˆ™ä¸ä¼šå†å‘åä¼ é€’itemã€‚æ­¤å¤–ï¼Œä¸€äº›è´¹æ—¶çš„æ“ä½œ(ä¾‹å¦‚æ’å…¥æ•°æ®åº“)å¯é€šè¿‡è¿”å›Deferredå¯¹è±¡å®ç°å¼‚æ­¥æ“ä½œã€‚
- ä¸Šè¿°ç»„ä»¶å‡å¯å®ç°open_spiderï¼Œclose_spiderå’Œfrom_crawlerç­‰æ–¹æ³•ï¼Œç”¨äºåœ¨æµç¨‹ä¸­çš„å¼€å§‹å’Œç»“æŸæ—¶ï¼Œä»¥åŠåˆå§‹åŒ–æ—¶è¿›è¡Œè‡ªå®šä¹‰è®¾ç½®æˆ–èµ„æºåˆå§‹åŒ–å’Œå›æ”¶æ“ä½œç­‰ï¼Œä½¿æ•´ä¸ªæµç¨‹æ›´"Gracefully"ã€‚
- å°ç»“:Scrapyæ˜¯ä¸€ä¸ªä¼˜ç§€çš„æ¡†æ¶ï¼Œæœ‰ç€ å‹å–„çš„æ–‡æ¡£å’Œæ´»è·ƒçš„ç¤¾åŒºã€‚å…¶å®æŠ›å¼€æ–‡æ¡£ï¼Œscrapyæºç å°±æ˜¯æœ€å¥½çš„æ–‡æ¡£ï¼Œä»é˜…è¯»ä»£ç ä¸­å¯ä»¥æ”¶è·ä¸å°‘scrapyä½¿ç”¨å’Œæ¡†æ¶è®¾è®¡çš„æŠ€å·§ã€‚çˆ¬è™«æ˜¯ä¸€ä»¶æœ‰æ„æ€çš„äº‹æƒ…ï¼Œä»å¼‚æ­¥åˆ°å¹¶å‘ï¼Œä»cookieç™»å½•åˆ°JSåŠ å¯†ï¼Œæ€»èƒ½å‘ç°æƒŠå–œã€‚å¸Œæœ›å¤§å®¶åœ¨è·å–æ•°æ®çš„åŒæ—¶ï¼Œä¹Ÿèƒ½æ”¶è·ä¸€ä»½ä¹è¶£ã€‚





â€‹			





















































â€‹	